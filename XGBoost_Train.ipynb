{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GISTDA Wildfire Machine Learning Training\n",
    "\n",
    "### This project is focused on developing and training machine learning models to predict and monitor wildfires. It utilizes datasets from Sentinel-2 Images as Raster GeoTIFF format which have been wildfire extraction, analyzes environmental factors, and applies machine learning algorithms to enhance prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **Numpy (`numpy`)**\n",
    "   - **Description**: A powerful library for numerical computations in Python. It provides support for arrays, matrices, and a large collection of mathematical functions to operate on these data structures, making it essential for scientific and statistical analysis.\n",
    "\n",
    "##### 2. **Pandas (`pandas`)**\n",
    "   - **Description**: A widely-used library for data manipulation and analysis. It provides data structures such as DataFrames to store and manipulate large datasets, making data cleaning, transformation, and exploration more efficient and intuitive.\n",
    "\n",
    "##### 3. **Matplotlib (`matplotlib.pyplot`)**\n",
    "   - **Description**: A plotting library that provides tools for creating a wide range of static, animated, and interactive visualizations in Python. It is commonly used for generating graphs, charts, and other visual data representations.\n",
    "\n",
    "##### 4. **Seaborn (`seaborn`)**\n",
    "   - **Description**: A statistical data visualization library built on top of Matplotlib. It offers high-level functions for creating informative and attractive visualizations, especially useful for exploring and understanding data trends and distributions.\n",
    "\n",
    "##### 5. **OS (`os`)**\n",
    "   - **Description**: A standard library in Python that provides functions to interact with the operating system, allowing you to work with directories, files, and system paths. It's useful for handling file operations, environment variables, and system commands.\n",
    "\n",
    "##### 6. **Pickle (`pickle`)**\n",
    "   - **Description**: A Python module used to serialize and deserialize Python objects, allowing you to save complex data structures to files and load them back into your program. It’s commonly used for saving trained models or intermediate data states.\n",
    "\n",
    "##### 7. **Dask (`dask.dataframe` and `dask`)**\n",
    "   - **Description**: A parallel computing library that scales up computations on larger datasets. `dask.dataframe` provides similar functionality to Pandas DataFrames but can handle larger-than-memory data by performing parallel, chunked computations.\n",
    "\n",
    "##### 8. **Rasterio (`rasterio`)**\n",
    "   - **Description**: A library for reading and writing geospatial raster data. It’s widely used for working with geospatial data in formats like GeoTIFF, allowing for operations on large image files commonly used in remote sensing and GIS applications.\n",
    "\n",
    "##### 9. **Scikit-Learn (`sklearn`)**\n",
    "   - **Description**: A robust library for machine learning that provides simple and efficient tools for data analysis and modeling. It includes modules like `MinMaxScaler` for scaling data, `cross_val_score` and `cross_val_predict` for evaluating models, and various classifiers.\n",
    "\n",
    "##### 10. **LightGBM (`lightgbm`)**\n",
    "   - **Description**: A high-performance, gradient-boosting framework developed by Microsoft. It's optimized for speed and efficiency on large datasets and is particularly well-suited for structured data and classification problems.\n",
    "\n",
    "##### 11. **IPython (`IPython.display`)**\n",
    "   - **Description**: A library for creating interactive elements in Jupyter Notebooks, such as displaying Markdown, HTML, and other rich content. It’s often used to improve the readability and interactivity of notebook outputs.\n",
    "\n",
    "##### 12. **Delayed and Compute (`dask.delayed` and `dask.compute`)**\n",
    "   - **Description**: Functions in the Dask library that allow you to parallelize and execute tasks asynchronously. `delayed` is used to mark a function for lazy evaluation, and `compute` is used to execute the delayed functions, making computations efficient and scalable.\n",
    "\n",
    "##### 13. **Rasterio Windows (`rasterio.windows`)**\n",
    "   - **Description**: A submodule in Rasterio that allows for windowed or tiled reading of raster data. This is useful for reading and processing only portions of large raster datasets, improving efficiency when working with large geospatial files.\n",
    "\n",
    "##### 14. **XGBoost (`xgboost`)**\n",
    "   - **Description**: An optimized gradient-boosting framework that’s highly effective for predictive modeling tasks, especially classification and regression. It’s known for its speed and performance and is widely used in machine learning competitions for its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import dask.dataframe as dd\n",
    "import rasterio\n",
    "import dask\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from IPython.display import display, Markdown\n",
    "from dask import delayed, compute\n",
    "from rasterio.windows import Window\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Raster GeoTIFF files\n",
    "\n",
    "This code reads large raster files in manageable chunks, processes each chunk, and stores the data in a consolidated DataFrame.\n",
    "\n",
    "1. **Folder Path and Chunk Size**: Sets the path to the folder containing raster files (`raster_train_file_path`) and defines the chunk size (`CHUNK_SIZE`).\n",
    "\n",
    "2. **`read_raster_in_chunks` Function**:\n",
    "   - Reads a raster file chunk-by-chunk.\n",
    "   - For each chunk, it extracts data from all bands, along with the pixel coordinates.\n",
    "   - Stores each chunk's data in a temporary DataFrame, which is added to a list.\n",
    "\n",
    "3. **Task Creation**:\n",
    "   - Creates a list of tasks using Dask's `@delayed` to process each raster file in parallel.\n",
    "\n",
    "4. **Compute and Combine**:\n",
    "   - Computes each task to get individual DataFrames for each file, then combines them into a final DataFrame (`final_df`).\n",
    "\n",
    "5. **Debug Outputs**:\n",
    "   - Prints the shape, columns, and a sample of the final DataFrame for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_all before normalization: (266534912, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B02</th>\n",
       "      <th>B03</th>\n",
       "      <th>B04</th>\n",
       "      <th>B05</th>\n",
       "      <th>B06</th>\n",
       "      <th>B07</th>\n",
       "      <th>B08</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>dNBR</th>\n",
       "      <th>BAIS2</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>Burn_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1544.0</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>1737.0</td>\n",
       "      <td>2066.0</td>\n",
       "      <td>2474.0</td>\n",
       "      <td>2656.0</td>\n",
       "      <td>2675.0</td>\n",
       "      <td>2919.0</td>\n",
       "      <td>3043.0</td>\n",
       "      <td>2457.0</td>\n",
       "      <td>0.228026</td>\n",
       "      <td>17610.087891</td>\n",
       "      <td>0.212602</td>\n",
       "      <td>-0.247377</td>\n",
       "      <td>0.425108</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1492.0</td>\n",
       "      <td>1596.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>2066.0</td>\n",
       "      <td>2474.0</td>\n",
       "      <td>2656.0</td>\n",
       "      <td>2716.0</td>\n",
       "      <td>2919.0</td>\n",
       "      <td>3043.0</td>\n",
       "      <td>2457.0</td>\n",
       "      <td>0.228026</td>\n",
       "      <td>17853.355469</td>\n",
       "      <td>0.232864</td>\n",
       "      <td>-0.259740</td>\n",
       "      <td>0.465623</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1494.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>1716.0</td>\n",
       "      <td>2068.0</td>\n",
       "      <td>2410.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>2708.0</td>\n",
       "      <td>2878.0</td>\n",
       "      <td>3055.0</td>\n",
       "      <td>2531.0</td>\n",
       "      <td>0.249016</td>\n",
       "      <td>12238.231445</td>\n",
       "      <td>0.224231</td>\n",
       "      <td>-0.251386</td>\n",
       "      <td>0.448362</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1512.0</td>\n",
       "      <td>1642.0</td>\n",
       "      <td>1768.0</td>\n",
       "      <td>2068.0</td>\n",
       "      <td>2410.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>2655.0</td>\n",
       "      <td>2878.0</td>\n",
       "      <td>3055.0</td>\n",
       "      <td>2531.0</td>\n",
       "      <td>0.249016</td>\n",
       "      <td>12056.858398</td>\n",
       "      <td>0.200543</td>\n",
       "      <td>-0.235746</td>\n",
       "      <td>0.400995</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1566.0</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>1791.0</td>\n",
       "      <td>2115.0</td>\n",
       "      <td>2491.0</td>\n",
       "      <td>2673.0</td>\n",
       "      <td>2586.0</td>\n",
       "      <td>2935.0</td>\n",
       "      <td>3187.0</td>\n",
       "      <td>2637.0</td>\n",
       "      <td>0.258770</td>\n",
       "      <td>9880.970703</td>\n",
       "      <td>0.181631</td>\n",
       "      <td>-0.214655</td>\n",
       "      <td>0.363180</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      B02     B03     B04     B05     B06     B07     B08     B8A     B11  \\\n",
       "0  1544.0  1614.0  1737.0  2066.0  2474.0  2656.0  2675.0  2919.0  3043.0   \n",
       "1  1492.0  1596.0  1690.0  2066.0  2474.0  2656.0  2716.0  2919.0  3043.0   \n",
       "2  1494.0  1620.0  1716.0  2068.0  2410.0  2682.0  2708.0  2878.0  3055.0   \n",
       "3  1512.0  1642.0  1768.0  2068.0  2410.0  2682.0  2655.0  2878.0  3055.0   \n",
       "4  1566.0  1672.0  1791.0  2115.0  2491.0  2673.0  2586.0  2935.0  3187.0   \n",
       "\n",
       "      B12      dNBR         BAIS2      NDVI      NDWI      SAVI  Burn_Label  \n",
       "0  2457.0  0.228026  17610.087891  0.212602 -0.247377  0.425108         0.0  \n",
       "1  2457.0  0.228026  17853.355469  0.232864 -0.259740  0.465623         0.0  \n",
       "2  2531.0  0.249016  12238.231445  0.224231 -0.251386  0.448362         0.0  \n",
       "3  2531.0  0.249016  12056.858398  0.200543 -0.235746  0.400995         0.0  \n",
       "4  2637.0  0.258770   9880.970703  0.181631 -0.214655  0.363180         0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "FOLDER_PATH = 'Raster_Train'\n",
    "\n",
    "# Find all .tif files in the folder\n",
    "tif_files = glob.glob(os.path.join(FOLDER_PATH, '*.tif'))\n",
    "\n",
    "# Load and concatenate all data\n",
    "all_data = []\n",
    "\n",
    "for file in tif_files:\n",
    "    with rasterio.open(file) as src:\n",
    "        bands = src.read()  # (bands, height, width)\n",
    "        bands_reshaped = bands.reshape(bands.shape[0], -1).T  # (pixels, bands)\n",
    "\n",
    "        # --- Dynamic Column Naming Logic ---\n",
    "        # Attempt to use band descriptions from the file's metadata\n",
    "        column_names = list(src.descriptions)\n",
    "\n",
    "        # Fallback if descriptions are not available, empty, or don't match the band count\n",
    "        if not all(column_names) or len(column_names) != src.count:\n",
    "            generic_names = [f'band_{i+1}' for i in range(src.count)]\n",
    "            # If there were some descriptions, but not a complete set, warn the user.\n",
    "            if any(column_names):\n",
    "                print(f\"Warning: Incomplete band descriptions in {FOLDER_PATH}. Falling back to generic names.\")\n",
    "            column_names = generic_names\n",
    "\n",
    "        df = pd.DataFrame(bands_reshaped, columns=column_names)\n",
    "        all_data.append(df)\n",
    "\n",
    "# Combine into a single DataFrame\n",
    "df_all = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Check df_all shape before normalization\n",
    "print(\"Shape of df_all before normalization:\", df_all.shape)\n",
    "\n",
    "# Display DataFrame (computing results)\n",
    "display(df_all.head())  # Displays a small, computed sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) & Feature Engineering\n",
    "\n",
    "This code performs several operations on a DataFrame (`final_df`), converting it to a Dask DataFrame for distributed processing, renaming columns, and dropping unnecessary columns:\n",
    "\n",
    "1. **Convert to Dask DataFrame**:\n",
    "   - Converts the existing Pandas DataFrame (`final_df`) into a Dask DataFrame (`ddf`) to enable parallel, distributed processing.\n",
    "\n",
    "2. **Rename Columns**:\n",
    "   - Defines a list of new column names for Sentinel-2 bands and other data (`new_col_names`).\n",
    "   - Renames `ddf` columns using this list to make them more descriptive.\n",
    "\n",
    "3. **Drop Unneeded Columns**:\n",
    "   - Drops columns such as `raster_file`, `subfolder`, `x`, `y`, and `dNBR`, keeping only essential information in the DataFrame (`df`).\n",
    "\n",
    "4. **Display DataFrame**:\n",
    "   - Uses `display()` to view the DataFrame when needed, triggering Dask's computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      B02     B03     B04     B05     B06     B07     B08     B8A     B11  \\\n",
      "0  1544.0  1614.0  1737.0  2066.0  2474.0  2656.0  2675.0  2919.0  3043.0   \n",
      "1  1492.0  1596.0  1690.0  2066.0  2474.0  2656.0  2716.0  2919.0  3043.0   \n",
      "2  1494.0  1620.0  1716.0  2068.0  2410.0  2682.0  2708.0  2878.0  3055.0   \n",
      "3  1512.0  1642.0  1768.0  2068.0  2410.0  2682.0  2655.0  2878.0  3055.0   \n",
      "4  1566.0  1672.0  1791.0  2115.0  2491.0  2673.0  2586.0  2935.0  3187.0   \n",
      "\n",
      "      B12         BAIS2      NDVI      NDWI      SAVI  Burn_Label  \n",
      "0  2457.0  17610.087891  0.212602 -0.247377  0.425108         0.0  \n",
      "1  2457.0  17853.355469  0.232864 -0.259740  0.465623         0.0  \n",
      "2  2531.0  12238.231445  0.224231 -0.251386  0.448362         0.0  \n",
      "3  2531.0  12056.858398  0.200543 -0.235746  0.400995         0.0  \n",
      "4  2637.0   9880.970703  0.181631 -0.214655  0.363180         0.0  \n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['dNBR']\n",
    "df = df_all.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display DataFrame (computing results)\n",
    "print(df.head())  # Displays a small, computed sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Burn Class\n",
    "\n",
    "This code checks and displays the counts of burn records in the DataFrame:\n",
    "\n",
    "1. **Count Burn Labels**:\n",
    "   - Counts occurrences in the `Burn_Label` column to determine the number of \"Burn\" and \"Unburn\" records.\n",
    "   - Renames the labels: `1` to \"Burn\" and `0` to \"Unburn\" for readability.\n",
    "\n",
    "2. **Display Counts**:\n",
    "   - Prints the resulting counts to show the distribution of burn and unburned areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burn_Label\n",
      "Unburn    233896598\n",
      "Burn       32638314\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check Burn Records\n",
    "burn_counts = df['Burn_Label'].value_counts().rename(index={1: 'Burn', 0: 'Unburn'})\n",
    "\n",
    "# Display the counts with labels\n",
    "print(burn_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "\n",
    "This code performs downsampling to balance the dataset by reducing the number of \"Unburn\" records to match the count of \"Burn\" records:\n",
    "\n",
    "1. **Get Burn Count**:\n",
    "   - Retrieves the count of \"Burn\" records from `burn_counts`.\n",
    "\n",
    "2. **Sample Unburned Records**:\n",
    "   - Selects a random sample of \"Unburn\" records, equal in size to the number of \"Burn\" records, using a fixed `random_state` for reproducibility.\n",
    "\n",
    "3. **Combine Burn and Downsampled Unburn Records**:\n",
    "   - Combines all \"Burn\" records with the downsampled \"Unburn\" sample into a new DataFrame (`downsampled_df`).\n",
    "\n",
    "4. **Check New Burn Record Counts**:\n",
    "   - Counts and displays the \"Burn\" and \"Unburn\" records in `downsampled_df` to verify balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burn_Label\n",
      "Burn      32638314\n",
      "Unburn    32638314\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "burn_count = burn_counts['Burn']\n",
    "unburn_sample = df[df['Burn_Label'] == 0].sample(n=burn_count, random_state=42)\n",
    "\n",
    "downsampled_df = pd.concat([df[df['Burn_Label'] == 1], unburn_sample])\n",
    "\n",
    "# Check Burn Records\n",
    "burn_counts = downsampled_df['Burn_Label'].value_counts().rename(index={1: 'Burn', 0: 'Unburn'})\n",
    "\n",
    "# Display the counts with labels\n",
    "print(burn_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove infinite values\n",
    "\n",
    "This code handles the presence of infinite values and missing data in the `downsampled_df` DataFrame:\n",
    "\n",
    "1. **Replace Infinite Values**:\n",
    "   - Replaces both positive and negative infinite values (`np.inf` and `-np.inf`) with `NaN` using `replace()`. This ensures that infinite values do not interfere with further processing.\n",
    "\n",
    "2. **Drop Rows with Missing Values**:\n",
    "   - Removes any rows containing `NaN` values using `dropna()`, ensuring the DataFrame only contains valid data.\n",
    "\n",
    "3. **Display the DataFrame**:\n",
    "   - Displays the cleaned DataFrame (`downsampled_df`) for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B02</th>\n",
       "      <th>B03</th>\n",
       "      <th>B04</th>\n",
       "      <th>B05</th>\n",
       "      <th>B06</th>\n",
       "      <th>B07</th>\n",
       "      <th>B08</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>BAIS2</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>Burn_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1570.0</td>\n",
       "      <td>1638.0</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>2036.0</td>\n",
       "      <td>2199.0</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>2348.0</td>\n",
       "      <td>2588.0</td>\n",
       "      <td>3156.0</td>\n",
       "      <td>2839.0</td>\n",
       "      <td>-12065.983398</td>\n",
       "      <td>0.135122</td>\n",
       "      <td>-0.178123</td>\n",
       "      <td>0.270179</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1587.0</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>1828.0</td>\n",
       "      <td>2036.0</td>\n",
       "      <td>2199.0</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>2345.0</td>\n",
       "      <td>2588.0</td>\n",
       "      <td>3156.0</td>\n",
       "      <td>2839.0</td>\n",
       "      <td>-11936.529297</td>\n",
       "      <td>0.123892</td>\n",
       "      <td>-0.168702</td>\n",
       "      <td>0.247724</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1598.0</td>\n",
       "      <td>1678.0</td>\n",
       "      <td>1812.0</td>\n",
       "      <td>2050.0</td>\n",
       "      <td>2219.0</td>\n",
       "      <td>2332.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2568.0</td>\n",
       "      <td>3203.0</td>\n",
       "      <td>2905.0</td>\n",
       "      <td>-15038.642578</td>\n",
       "      <td>0.139601</td>\n",
       "      <td>-0.177048</td>\n",
       "      <td>0.279136</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1546.0</td>\n",
       "      <td>1630.0</td>\n",
       "      <td>1734.0</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>2164.0</td>\n",
       "      <td>2327.0</td>\n",
       "      <td>2285.0</td>\n",
       "      <td>2482.0</td>\n",
       "      <td>2861.0</td>\n",
       "      <td>2461.0</td>\n",
       "      <td>-1882.131836</td>\n",
       "      <td>0.137099</td>\n",
       "      <td>-0.167305</td>\n",
       "      <td>0.274129</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1662.0</td>\n",
       "      <td>1766.0</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>2113.0</td>\n",
       "      <td>2255.0</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>2468.0</td>\n",
       "      <td>2532.0</td>\n",
       "      <td>2984.0</td>\n",
       "      <td>2631.0</td>\n",
       "      <td>-6323.865723</td>\n",
       "      <td>0.129519</td>\n",
       "      <td>-0.165801</td>\n",
       "      <td>0.258980</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199616788</th>\n",
       "      <td>1836.0</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>2470.0</td>\n",
       "      <td>2698.0</td>\n",
       "      <td>2894.0</td>\n",
       "      <td>2963.0</td>\n",
       "      <td>3065.0</td>\n",
       "      <td>3939.0</td>\n",
       "      <td>3423.0</td>\n",
       "      <td>-18058.707031</td>\n",
       "      <td>0.153592</td>\n",
       "      <td>-0.199110</td>\n",
       "      <td>0.307123</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244231951</th>\n",
       "      <td>1487.0</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>1795.0</td>\n",
       "      <td>2469.0</td>\n",
       "      <td>3049.0</td>\n",
       "      <td>3202.0</td>\n",
       "      <td>3402.0</td>\n",
       "      <td>3540.0</td>\n",
       "      <td>3413.0</td>\n",
       "      <td>2404.0</td>\n",
       "      <td>60252.687500</td>\n",
       "      <td>0.309217</td>\n",
       "      <td>-0.338316</td>\n",
       "      <td>0.618315</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559136</th>\n",
       "      <td>1701.0</td>\n",
       "      <td>1790.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>2171.0</td>\n",
       "      <td>2288.0</td>\n",
       "      <td>2368.0</td>\n",
       "      <td>2428.0</td>\n",
       "      <td>2556.0</td>\n",
       "      <td>3405.0</td>\n",
       "      <td>3115.0</td>\n",
       "      <td>-22166.927734</td>\n",
       "      <td>0.097153</td>\n",
       "      <td>-0.151257</td>\n",
       "      <td>0.194262</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103705560</th>\n",
       "      <td>1502.0</td>\n",
       "      <td>1808.0</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>2189.0</td>\n",
       "      <td>2956.0</td>\n",
       "      <td>3246.0</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>3454.0</td>\n",
       "      <td>3498.0</td>\n",
       "      <td>2567.0</td>\n",
       "      <td>46237.296875</td>\n",
       "      <td>0.306458</td>\n",
       "      <td>-0.274478</td>\n",
       "      <td>0.612790</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57420124</th>\n",
       "      <td>1852.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>2522.0</td>\n",
       "      <td>2718.0</td>\n",
       "      <td>2893.0</td>\n",
       "      <td>3050.0</td>\n",
       "      <td>3322.0</td>\n",
       "      <td>3441.0</td>\n",
       "      <td>4603.0</td>\n",
       "      <td>3570.0</td>\n",
       "      <td>-8812.741211</td>\n",
       "      <td>0.136893</td>\n",
       "      <td>-0.218635</td>\n",
       "      <td>0.273738</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59674126 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              B02     B03     B04     B05     B06     B07     B08     B8A  \\\n",
       "12         1570.0  1638.0  1789.0  2036.0  2199.0  2358.0  2348.0  2588.0   \n",
       "13         1587.0  1668.0  1828.0  2036.0  2199.0  2358.0  2345.0  2588.0   \n",
       "15         1598.0  1678.0  1812.0  2050.0  2219.0  2332.0  2400.0  2568.0   \n",
       "18         1546.0  1630.0  1734.0  1973.0  2164.0  2327.0  2285.0  2482.0   \n",
       "232        1662.0  1766.0  1902.0  2113.0  2255.0  2358.0  2468.0  2532.0   \n",
       "...           ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "199616788  1836.0  1979.0  2174.0  2470.0  2698.0  2894.0  2963.0  3065.0   \n",
       "244231951  1487.0  1682.0  1795.0  2469.0  3049.0  3202.0  3402.0  3540.0   \n",
       "3559136    1701.0  1790.0  1998.0  2171.0  2288.0  2368.0  2428.0  2556.0   \n",
       "103705560  1502.0  1808.0  1686.0  2189.0  2956.0  3246.0  3176.0  3454.0   \n",
       "57420124   1852.0  2130.0  2522.0  2718.0  2893.0  3050.0  3322.0  3441.0   \n",
       "\n",
       "              B11     B12         BAIS2      NDVI      NDWI      SAVI  \\\n",
       "12         3156.0  2839.0 -12065.983398  0.135122 -0.178123  0.270179   \n",
       "13         3156.0  2839.0 -11936.529297  0.123892 -0.168702  0.247724   \n",
       "15         3203.0  2905.0 -15038.642578  0.139601 -0.177048  0.279136   \n",
       "18         2861.0  2461.0  -1882.131836  0.137099 -0.167305  0.274129   \n",
       "232        2984.0  2631.0  -6323.865723  0.129519 -0.165801  0.258980   \n",
       "...           ...     ...           ...       ...       ...       ...   \n",
       "199616788  3939.0  3423.0 -18058.707031  0.153592 -0.199110  0.307123   \n",
       "244231951  3413.0  2404.0  60252.687500  0.309217 -0.338316  0.618315   \n",
       "3559136    3405.0  3115.0 -22166.927734  0.097153 -0.151257  0.194262   \n",
       "103705560  3498.0  2567.0  46237.296875  0.306458 -0.274478  0.612790   \n",
       "57420124   4603.0  3570.0  -8812.741211  0.136893 -0.218635  0.273738   \n",
       "\n",
       "           Burn_Label  \n",
       "12                1.0  \n",
       "13                1.0  \n",
       "15                1.0  \n",
       "18                1.0  \n",
       "232               1.0  \n",
       "...               ...  \n",
       "199616788         0.0  \n",
       "244231951         0.0  \n",
       "3559136           0.0  \n",
       "103705560         0.0  \n",
       "57420124          0.0  \n",
       "\n",
       "[59674126 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replacing infinite with nan \n",
    "downsampled_df.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "  \n",
    "# Dropping all the rows with nan values \n",
    "downsampled_df.dropna(inplace=True)\n",
    "\n",
    "# Printing df \n",
    "display(downsampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate Burn_Label from DataFrame\n",
    "\n",
    "This code separates the `Burn_Label` from the main DataFrame and ensures the label is in the correct format:\n",
    "\n",
    "1. **Separate Burn Label**:\n",
    "   - Extracts the `Burn_Label` column from `downsampled_df` into a new DataFrame (`burn_label`).\n",
    "\n",
    "2. **Remove Burn Label from Main DataFrame**:\n",
    "   - Drops the `Burn_Label` column from `downsampled_df` to ensure only the feature data remains.\n",
    "\n",
    "3. **Convert Burn Label to Integer**:\n",
    "   - Changes the data type of the `burn_label` DataFrame to `int32` to ensure consistent and efficient processing.\n",
    "\n",
    "4. **Display Burn Label**:\n",
    "   - Displays the modified `burn_label` DataFrame to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Burn_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199616788</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244231951</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559136</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103705560</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57420124</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59674126 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Burn_Label\n",
       "12                  1\n",
       "13                  1\n",
       "15                  1\n",
       "18                  1\n",
       "232                 1\n",
       "...               ...\n",
       "199616788           0\n",
       "244231951           0\n",
       "3559136             0\n",
       "103705560           0\n",
       "57420124            0\n",
       "\n",
       "[59674126 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Seperate Burn_Label from DataFrame\n",
    "burn_label = downsampled_df[['Burn_Label']]\n",
    "\n",
    "# Drop Label from DataFrame\n",
    "downsampled_df = downsampled_df.drop(columns=['Burn_Label'])\n",
    "\n",
    "# Change type of Label to Integer Format\n",
    "burn_label = burn_label.astype('int32')\n",
    "display(burn_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Data with MinMax Scaler\n",
    "\n",
    "This code normalizes the feature data and saves the normalization model, while also combining it with the `Burn_Label`:\n",
    "\n",
    "1. **List Columns**:\n",
    "   - Creates a list of column names from `downsampled_df` to keep track of the column order after normalization (`cols_norm`).\n",
    "\n",
    "2. **Normalize the Data**:\n",
    "   - Imports `MinMaxScaler` from `sklearn` and fits it to the data in `downsampled_df`, which scales the values between 0 and 1.\n",
    "\n",
    "3. **Save the Scaler**:\n",
    "   - Saves the fitted `MinMaxScaler` model to a specified path (`MinMax_Scaler.pkl`) for later use.\n",
    "\n",
    "4. **Apply Normalization**:\n",
    "   - Normalizes the data by applying the `scaler` to `downsampled_df`, then converts the result back into a DataFrame (`df_norm`) with the original column names.\n",
    "\n",
    "5. **Check Shape**:\n",
    "   - Prints the shape of `df_norm` to confirm the normalization was applied correctly.\n",
    "\n",
    "6. **Concatenate with Burn Label**:\n",
    "   - Combines the normalized feature data (`df_norm`) with the `burn_label` DataFrame, aligning them by their indices and ensuring the result is a complete dataset.\n",
    "\n",
    "7. **Display the DataFrame**:\n",
    "   - Displays the final DataFrame (`df_norm`), which now includes both the normalized features and the `Burn_Label`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_norm after normalization: (59674126, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B02</th>\n",
       "      <th>B03</th>\n",
       "      <th>B04</th>\n",
       "      <th>B05</th>\n",
       "      <th>B06</th>\n",
       "      <th>B07</th>\n",
       "      <th>B08</th>\n",
       "      <th>B8A</th>\n",
       "      <th>B11</th>\n",
       "      <th>B12</th>\n",
       "      <th>BAIS2</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>NDWI</th>\n",
       "      <th>SAVI</th>\n",
       "      <th>Burn_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.111442</td>\n",
       "      <td>0.052246</td>\n",
       "      <td>0.065184</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.118952</td>\n",
       "      <td>0.142207</td>\n",
       "      <td>0.181734</td>\n",
       "      <td>0.166144</td>\n",
       "      <td>0.136634</td>\n",
       "      <td>0.120112</td>\n",
       "      <td>0.281439</td>\n",
       "      <td>0.656207</td>\n",
       "      <td>0.292231</td>\n",
       "      <td>0.656066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.112649</td>\n",
       "      <td>0.054799</td>\n",
       "      <td>0.068292</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.118952</td>\n",
       "      <td>0.142207</td>\n",
       "      <td>0.181502</td>\n",
       "      <td>0.166144</td>\n",
       "      <td>0.136634</td>\n",
       "      <td>0.120112</td>\n",
       "      <td>0.281524</td>\n",
       "      <td>0.649715</td>\n",
       "      <td>0.297891</td>\n",
       "      <td>0.649573</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.113430</td>\n",
       "      <td>0.055650</td>\n",
       "      <td>0.067017</td>\n",
       "      <td>0.094183</td>\n",
       "      <td>0.120741</td>\n",
       "      <td>0.139792</td>\n",
       "      <td>0.185759</td>\n",
       "      <td>0.164300</td>\n",
       "      <td>0.139576</td>\n",
       "      <td>0.124315</td>\n",
       "      <td>0.279468</td>\n",
       "      <td>0.658797</td>\n",
       "      <td>0.292877</td>\n",
       "      <td>0.658656</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.109739</td>\n",
       "      <td>0.051566</td>\n",
       "      <td>0.060802</td>\n",
       "      <td>0.087673</td>\n",
       "      <td>0.115821</td>\n",
       "      <td>0.139328</td>\n",
       "      <td>0.176858</td>\n",
       "      <td>0.156371</td>\n",
       "      <td>0.118170</td>\n",
       "      <td>0.096039</td>\n",
       "      <td>0.288188</td>\n",
       "      <td>0.657350</td>\n",
       "      <td>0.298730</td>\n",
       "      <td>0.657209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.117973</td>\n",
       "      <td>0.063138</td>\n",
       "      <td>0.074189</td>\n",
       "      <td>0.099510</td>\n",
       "      <td>0.123960</td>\n",
       "      <td>0.142207</td>\n",
       "      <td>0.191022</td>\n",
       "      <td>0.160981</td>\n",
       "      <td>0.125868</td>\n",
       "      <td>0.106865</td>\n",
       "      <td>0.285244</td>\n",
       "      <td>0.652969</td>\n",
       "      <td>0.299634</td>\n",
       "      <td>0.652827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59674121</th>\n",
       "      <td>0.130324</td>\n",
       "      <td>0.081263</td>\n",
       "      <td>0.095864</td>\n",
       "      <td>0.129692</td>\n",
       "      <td>0.163581</td>\n",
       "      <td>0.191993</td>\n",
       "      <td>0.229334</td>\n",
       "      <td>0.210124</td>\n",
       "      <td>0.185642</td>\n",
       "      <td>0.157305</td>\n",
       "      <td>0.277467</td>\n",
       "      <td>0.666885</td>\n",
       "      <td>0.279623</td>\n",
       "      <td>0.666750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59674122</th>\n",
       "      <td>0.105551</td>\n",
       "      <td>0.055990</td>\n",
       "      <td>0.065663</td>\n",
       "      <td>0.129608</td>\n",
       "      <td>0.194974</td>\n",
       "      <td>0.220602</td>\n",
       "      <td>0.263313</td>\n",
       "      <td>0.253918</td>\n",
       "      <td>0.152720</td>\n",
       "      <td>0.092409</td>\n",
       "      <td>0.329368</td>\n",
       "      <td>0.756851</td>\n",
       "      <td>0.195993</td>\n",
       "      <td>0.756741</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59674123</th>\n",
       "      <td>0.120741</td>\n",
       "      <td>0.065180</td>\n",
       "      <td>0.081839</td>\n",
       "      <td>0.104413</td>\n",
       "      <td>0.126912</td>\n",
       "      <td>0.143136</td>\n",
       "      <td>0.187926</td>\n",
       "      <td>0.163194</td>\n",
       "      <td>0.152219</td>\n",
       "      <td>0.137689</td>\n",
       "      <td>0.274744</td>\n",
       "      <td>0.634258</td>\n",
       "      <td>0.308371</td>\n",
       "      <td>0.634112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59674124</th>\n",
       "      <td>0.106616</td>\n",
       "      <td>0.066712</td>\n",
       "      <td>0.056977</td>\n",
       "      <td>0.105935</td>\n",
       "      <td>0.186656</td>\n",
       "      <td>0.224689</td>\n",
       "      <td>0.245820</td>\n",
       "      <td>0.245989</td>\n",
       "      <td>0.158040</td>\n",
       "      <td>0.102789</td>\n",
       "      <td>0.320079</td>\n",
       "      <td>0.755256</td>\n",
       "      <td>0.234344</td>\n",
       "      <td>0.755143</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59674125</th>\n",
       "      <td>0.131459</td>\n",
       "      <td>0.094112</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>0.150659</td>\n",
       "      <td>0.181021</td>\n",
       "      <td>0.206483</td>\n",
       "      <td>0.257121</td>\n",
       "      <td>0.244791</td>\n",
       "      <td>0.227202</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.283595</td>\n",
       "      <td>0.657231</td>\n",
       "      <td>0.267893</td>\n",
       "      <td>0.657095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59674126 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               B02       B03       B04       B05       B06       B07  \\\n",
       "0         0.111442  0.052246  0.065184  0.093000  0.118952  0.142207   \n",
       "1         0.112649  0.054799  0.068292  0.093000  0.118952  0.142207   \n",
       "2         0.113430  0.055650  0.067017  0.094183  0.120741  0.139792   \n",
       "3         0.109739  0.051566  0.060802  0.087673  0.115821  0.139328   \n",
       "4         0.117973  0.063138  0.074189  0.099510  0.123960  0.142207   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "59674121  0.130324  0.081263  0.095864  0.129692  0.163581  0.191993   \n",
       "59674122  0.105551  0.055990  0.065663  0.129608  0.194974  0.220602   \n",
       "59674123  0.120741  0.065180  0.081839  0.104413  0.126912  0.143136   \n",
       "59674124  0.106616  0.066712  0.056977  0.105935  0.186656  0.224689   \n",
       "59674125  0.131459  0.094112  0.123596  0.150659  0.181021  0.206483   \n",
       "\n",
       "               B08       B8A       B11       B12     BAIS2      NDVI  \\\n",
       "0         0.181734  0.166144  0.136634  0.120112  0.281439  0.656207   \n",
       "1         0.181502  0.166144  0.136634  0.120112  0.281524  0.649715   \n",
       "2         0.185759  0.164300  0.139576  0.124315  0.279468  0.658797   \n",
       "3         0.176858  0.156371  0.118170  0.096039  0.288188  0.657350   \n",
       "4         0.191022  0.160981  0.125868  0.106865  0.285244  0.652969   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "59674121  0.229334  0.210124  0.185642  0.157305  0.277467  0.666885   \n",
       "59674122  0.263313  0.253918  0.152720  0.092409  0.329368  0.756851   \n",
       "59674123  0.187926  0.163194  0.152219  0.137689  0.274744  0.634258   \n",
       "59674124  0.245820  0.245989  0.158040  0.102789  0.320079  0.755256   \n",
       "59674125  0.257121  0.244791  0.227202  0.166667  0.283595  0.657231   \n",
       "\n",
       "              NDWI      SAVI  Burn_Label  \n",
       "0         0.292231  0.656066           1  \n",
       "1         0.297891  0.649573           1  \n",
       "2         0.292877  0.658656           1  \n",
       "3         0.298730  0.657209           1  \n",
       "4         0.299634  0.652827           1  \n",
       "...            ...       ...         ...  \n",
       "59674121  0.279623  0.666750           0  \n",
       "59674122  0.195993  0.756741           0  \n",
       "59674123  0.308371  0.634112           0  \n",
       "59674124  0.234344  0.755143           0  \n",
       "59674125  0.267893  0.657095           0  \n",
       "\n",
       "[59674126 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reassign the dataframe with a list of the columns\n",
    "cols_norm = downsampled_df.columns.tolist()\n",
    "\n",
    "# Import Normalize technique\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize data\n",
    "scaler.fit(downsampled_df)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_save_path = r'Export_Model'\n",
    "save_path = os.path.join(scaler_save_path, 'MinMax_Scaler.pkl')\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Normalize Data\n",
    "df_norm = scaler.transform(downsampled_df)\n",
    "df_norm = pd.DataFrame(df_norm, columns=cols_norm)\n",
    "\n",
    "# Check df_norm shape after normalization\n",
    "print(\"Shape of df_norm after normalization:\", df_norm.shape)\n",
    "\n",
    "# Concatenate df_norm with burn_label\n",
    "df_norm = pd.concat([df_norm.reset_index(drop=True), burn_label.reset_index(drop=True)], axis=1, sort=False)\n",
    "display(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models Training Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Training\n",
    "\n",
    "The `xgboost_model` function performs hyperparameter tuning for the XGBoost model to find the optimal `max_depth` and evaluate its performance using cross-validation. GPU processing is enabled to speed up the training process. Below is a breakdown of the steps involved:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `df`: The DataFrame containing the dataset with the target variable `Burn_Label`.\n",
    "   - `xgb_estimator_num`: The number of estimators (trees) for the XGBoost model, default value is 100.\n",
    "   - `objective`: The objective function for the model, default value is `\"binary:logistic\"` for binary classification.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - The target variable (`Burn_Label`) is separated from the features, and the feature matrix (`X`) and target vector (`Y`) are defined.\n",
    "\n",
    "3. **Cross-Validation Setup**:\n",
    "   - A 10-fold cross-validation strategy (`KFold`) is used to split the data into training and validation sets for robust evaluation of the model.\n",
    "\n",
    "4. **Hyperparameter Tuning**:\n",
    "   - The function iterates through different values of `max_depth` (ranging from 2 to 20), training an XGBoost model with each `max_depth` value.\n",
    "   - For each value of `max_depth`, the model is evaluated using cross-validation, and the mean cross-validation accuracy is stored.\n",
    "\n",
    "5. **GPU Acceleration**:\n",
    "   - GPU processing is enabled by using the `tree_method=\"gpu_hist\"` and `predictor=\"gpu_predictor\"` parameters for both training and prediction. This significantly accelerates the model training and prediction times.\n",
    "\n",
    "6. **Plotting Results**:\n",
    "   - A plot is generated to visualize the relationship between `max_depth` and mean cross-validation accuracy. This helps in identifying the optimal `max_depth` for the best model.\n",
    "\n",
    "7. **Optimal Model Training**:\n",
    "   - Once the best `max_depth` is identified, the final model is trained with the optimal hyperparameters, and cross-validation is performed again to calculate the mean and standard deviation of the accuracy.\n",
    "\n",
    "8. **Model Evaluation**:\n",
    "   - A classification report and confusion matrix are generated to evaluate the model's performance in terms of precision, recall, F1-score, and overall accuracy.\n",
    "   - A confusion matrix heatmap is plotted to provide a clear visualization of the model's predictions versus the actual outcomes.\n",
    "\n",
    "9. **Output**:\n",
    "   - The function returns the best-trained XGBoost model (`xgb_model_best`), along with the mean and standard deviation of the cross-validation scores.\n",
    "\n",
    "10. **Example Usage**:\n",
    "   - The function is called with a dataset (`df_norm`), and it performs hyperparameter tuning and evaluation of the XGBoost model using GPU acceleration.\n",
    "\n",
    "This approach leverages GPU resources for faster training while optimizing model performance based on cross-validation results, making it suitable for large datasets and efficient model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Hyperparameter Tuning for XGBoost"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\wildfire_env\\Lib\\site-packages\\xgboost\\core.py:705: UserWarning: [09:04:27] WARNING: D:\\bld\\xgboost-split_1748292846627\\work\\src\\common\\error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth: 2, CV Score: 98.37%, Std Dev: 0.01%\n",
      "Max Depth: 3, CV Score: 98.38%, Std Dev: 0.0%\n",
      "Max Depth: 4, CV Score: 98.4%, Std Dev: 0.0%\n",
      "Max Depth: 5, CV Score: 98.42%, Std Dev: 0.0%\n",
      "Max Depth: 6, CV Score: 98.44%, Std Dev: 0.01%\n",
      "Max Depth: 7, CV Score: 98.46%, Std Dev: 0.01%\n"
     ]
    }
   ],
   "source": [
    "def xgboost_model(df_norm, xgb_estimator_num=100, objective=\"binary:logistic\"):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning for XGBoost to find optimal max_depth\n",
    "    and plots the results with GPU or CPU processing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_norm : pandas.DataFrame\n",
    "        Input dataframe containing features and target variable\n",
    "    xgb_estimator_num : int, optional (default=100)\n",
    "        Number of estimators for XGBoost\n",
    "    objective : str, optional (default=\"binary:logistic\")\n",
    "        Objective function for XGBoost\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (best_xgb_model, mean_cross_val_score, std_cross_val_score)\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    def is_gpu_available():\n",
    "        try:\n",
    "            test_model = XGBClassifier(tree_method=\"gpu_hist\")\n",
    "            return \"gpu_hist\" in test_model.get_xgb_params().get(\"tree_method\", \"\")\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    gpu_available = is_gpu_available()\n",
    "    if gpu_available:\n",
    "        print(f\"GPU available: {gpu_available}\")\n",
    "    else:\n",
    "        print(f\"GPU unavailable, CPU Enabled.\")\n",
    "\n",
    "    display(Markdown(\"### Hyperparameter Tuning for XGBoost\"))\n",
    "    print()  # Add Blank Line\n",
    "\n",
    "    # Define the features and target\n",
    "    X = df_norm.drop(columns=['Burn_Label'])  # Features: all columns except Burn_Label\n",
    "    Y = df_norm['Burn_Label']                 # Target: Burn_Label column\n",
    "\n",
    "    # Set up cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)  # 10-fold cross-validation\n",
    "    \n",
    "    max_depths = list(range(2, 21))  # Range of max_depth to test\n",
    "    mean_cv_scores = []\n",
    "    cv_std_devs = []\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    for max_depth in max_depths:\n",
    "        xgb_model = XGBClassifier(\n",
    "            n_estimators=xgb_estimator_num,\n",
    "            max_depth=max_depth,\n",
    "            objective=objective,\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42,\n",
    "            tree_method=\"hist\",  # Use \"hist\" for both CPU and GPU\n",
    "            device=\"cuda\" if gpu_available else \"cpu\"  # Explicitly set device\n",
    "        )\n",
    "        try:\n",
    "            scores_cv = cross_val_score(xgb_model, X, Y, cv=kf, scoring='accuracy')\n",
    "            mean_cv_scores.append(scores_cv.mean())\n",
    "            cv_std_devs.append(scores_cv.std())\n",
    "            print(f\"Max Depth: {max_depth}, CV Score: {round(scores_cv.mean() * 100, 2)}%, Std Dev: {round(scores_cv.std() * 100, 2)}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing max_depth {max_depth}: {e}\")\n",
    "            mean_cv_scores.append(0)\n",
    "            \n",
    "    # Find the best max_depth\n",
    "    best_max_depth = max_depths[np.argmax(mean_cv_scores)]\n",
    "    best_accuracy = max(mean_cv_scores)\n",
    "    print(f\"Best Max Depth: {best_max_depth} with Mean CV Accuracy: {round(best_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Plotting the mean cross-validation accuracy vs. max_depth\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(max_depths, mean_cv_scores, marker='o', linestyle='-', color='blue')\n",
    "    plt.fill_between(\n",
    "        max_depths, \n",
    "        np.array(mean_cv_scores) - np.array(cv_std_devs), \n",
    "        np.array(mean_cv_scores) + np.array(cv_std_devs), \n",
    "        color='lightblue', alpha=0.5, label='Std Dev'\n",
    "    )\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Mean CV Accuracy')\n",
    "    plt.title('XGBoost - Max Depth vs. Mean CV Accuracy')\n",
    "    plt.xticks(max_depths)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Train and evaluate XGBoost with the best max_depth\n",
    "    xgb_model_best = XGBClassifier(\n",
    "        n_estimators=xgb_estimator_num,\n",
    "        max_depth=best_max_depth,\n",
    "        objective=objective,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",  # Use \"hist\" for both CPU and GPU\n",
    "        device=\"cuda\" if gpu_available else \"cpu\"  # Explicitly set device\n",
    "    )\n",
    "    xgb_model_best.fit(X, Y)\n",
    "    scores_cv = cross_val_score(xgb_model_best, X, Y, cv=kf, scoring='accuracy')\n",
    "    mean_cv = scores_cv.mean()\n",
    "    std_cv = scores_cv.std()\n",
    "\n",
    "    # Display Cross-validation results\n",
    "    print(f\"XGBoost (Best Max Depth = {best_max_depth}) Cross-validation scores: {round(mean_cv * 100, 2)}%\")\n",
    "    print(f\"XGBoost (Best Max Depth = {best_max_depth}) Standard deviation: {round(std_cv * 100, 2)}%\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    y_pred = cross_val_predict(xgb_model_best, X, Y, cv=kf)\n",
    "    report = classification_report(Y, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(Y, y_pred)\n",
    "    \n",
    "    xgb_result = [{\n",
    "        'Classifier': 'XGBoost',\n",
    "        'Model Definition': xgb_model_best,\n",
    "        'Class 0 - Precision': report['0']['precision'],\n",
    "        'Class 0 - Recall': report['0']['recall'],\n",
    "        'Class 0 - F1-Score': report['0']['f1-score'],\n",
    "        'Class 1 - Precision': report['1']['precision'],\n",
    "        'Class 1 - Recall': report['1']['recall'],\n",
    "        'Class 1 - F1-Score': report['1']['f1-score'],\n",
    "        'Average - Precision': report['macro avg']['precision'],\n",
    "        'Average - Recall': report['macro avg']['recall'],\n",
    "        'Average - F1-Score': report['macro avg']['f1-score'],\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Confusion Matrix': cm\n",
    "    }]\n",
    "    \n",
    "    xgb_result_df = pd.DataFrame(xgb_result)\n",
    "    \n",
    "    display(Markdown(\"### Classification Report of XGBoost (Best Max Depth)\"))\n",
    "    display(xgb_result_df)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix - XGBoost (Best Max Depth)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    print()  # Add Blank Line\n",
    "    display(Markdown(\"<span style='color: green; font-weight: bold;'>XGBoost Model Run Complete (GPU/CPU Enabled)</span>\"))\n",
    "    print()  # Add Blank Line\n",
    "\n",
    "    return xgb_model_best, mean_cv, std_cv\n",
    "\n",
    "# Call the xgboost_model function with the desired parameters and your DataFrame\n",
    "xgb_model_best, mean_cv_best, std_cv_best = xgboost_model(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export XGBoost as pickle\n",
    "\n",
    "After training the XGBoost model with the optimal hyperparameters, the model is saved to a file for later use. The following steps are involved in saving the trained model:\n",
    "\n",
    "1. **Define Save Path**:\n",
    "   - The variable `savepath` is set to a directory called `'Export_Model'`, where the model will be saved. If the directory does not exist, it should be created manually or via additional code.\n",
    "\n",
    "2. **Create the File Path**:\n",
    "   - `xgb_filename_model` constructs the full file path for saving the model. The model file will be named `Model_XGB.sav` and will be stored in the `Export_Model` directory.\n",
    "\n",
    "3. **Save the Model**:\n",
    "   - The `pickle.dump` function is used to serialize and save the trained XGBoost model (`xgb_model_best`) to the specified file path. The file is saved in binary format using the `wb` mode.\n",
    "\n",
    "4. **Usage**:\n",
    "   - This allows the trained model to be loaded later for predictions or further analysis without needing to retrain it.\n",
    "\n",
    "By saving the model, you can easily load it in the future for inference or to integrate it into a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = r'Export_Model'\n",
    "xgb_filename_model = os.path.join(savepath, 'Model_XGB.sav')\n",
    "pickle.dump(xgb_model_best, open(xgb_filename_model, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildfire_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
