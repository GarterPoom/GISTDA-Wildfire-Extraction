{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GISTDA Wildfire Machine Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Read SHAPE File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import rasterio\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from IPython.display import display, Markdown\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask import delayed, compute\n",
    "from rasterio.windows import Window\n",
    "\n",
    "#pd.set_option(\"display.max_columns\", None)  # To show all columns in a pandas DataFrame\n",
    "\n",
    "# Define the folder containing the raster files\n",
    "raster_train_file_path = r'Raster\\output'\n",
    "\n",
    "# Parameters for chunk size\n",
    "CHUNK_SIZE = 1024\n",
    "\n",
    "@delayed\n",
    "def read_raster_in_chunks(raster_path, file, root):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        height, width = src.height, src.width\n",
    "        num_bands = src.count\n",
    "        band_names = [f'B{str(i).zfill(2)}' for i in range(1, num_bands + 1)]\n",
    "        chunk_dfs = []\n",
    "        \n",
    "        # Loop over the raster in chunks\n",
    "        for row in range(0, height, CHUNK_SIZE):\n",
    "            for col in range(0, width, CHUNK_SIZE):\n",
    "                window = Window(col_off=col, row_off=row, \n",
    "                              width=min(CHUNK_SIZE, width - col),\n",
    "                              height=min(CHUNK_SIZE, height - row))\n",
    "                \n",
    "                # Read all bands at once\n",
    "                data = src.read(window=window)\n",
    "                \n",
    "                # Check if chunk contains any data\n",
    "                if np.any(data):\n",
    "                    rows, cols = data[0].shape\n",
    "                    \n",
    "                    # Create base DataFrame with coordinates\n",
    "                    row_coords, col_coords = np.meshgrid(\n",
    "                        np.arange(row, row + rows),\n",
    "                        np.arange(col, col + cols),\n",
    "                        indexing=\"ij\"\n",
    "                    )\n",
    "                    \n",
    "                    chunk_df = pd.DataFrame({\n",
    "                        'raster_file': file,\n",
    "                        'subfolder': os.path.basename(root),\n",
    "                        'x': row_coords.flatten(),\n",
    "                        'y': col_coords.flatten()\n",
    "                    })\n",
    "                    \n",
    "                    # Add each band's data\n",
    "                    for band_idx, band_name in enumerate(band_names, 1):\n",
    "                        chunk_df[band_name] = data[band_idx-1].flatten()\n",
    "                    \n",
    "                    chunk_dfs.append(chunk_df)\n",
    "        \n",
    "        return pd.concat(chunk_dfs, ignore_index=True) if chunk_dfs else pd.DataFrame()\n",
    "\n",
    "# Create list of tasks\n",
    "dask_dfs = [\n",
    "    read_raster_in_chunks(os.path.join(root, file), file, root)\n",
    "    for root, dirs, files in os.walk(raster_train_file_path)\n",
    "    for file in files if file.endswith('.tif')\n",
    "]\n",
    "\n",
    "# Compute all tasks\n",
    "dataframes = compute(*dask_dfs)\n",
    "\n",
    "# Combine all DataFrames\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Debug prints\n",
    "print(\"DataFrame shape:\", final_df.shape)\n",
    "print(\"\\nDataFrame columns:\", final_df.columns.tolist())\n",
    "print(\"\\nSample of data:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrame to dask DataFrame if already loaded\n",
    "ddf = pd.DataFrame(final_df)  # Adjust number of partitions as needed\n",
    "\n",
    "## Rename Sentinel-2 Bands columns and Burn Label\n",
    "# List of new column names\n",
    "new_col_names = ['raster_file', 'subfolder', 'x', 'y', 'Band_1', 'Band_2', 'Band_3', 'Band_4', 'Band_5', \n",
    "                 'Band_6', 'Band_7', 'Band_8', 'Band_8A', 'Band_9', 'Band_11', 'Band_12', 'dNBR',\n",
    "                 'NDVI', 'NDWI', 'Burn_Label']\n",
    "\n",
    "\n",
    "# Renaming columns using the list\n",
    "ddf.columns = new_col_names\n",
    "\n",
    "# Drop columns in dask\n",
    "df = ddf.drop(columns=['raster_file', 'subfolder', 'x', 'y', 'dNBR'])\n",
    "display(df)  # Compute only when you need to display or save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Burn Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Burn Records\n",
    "burn_counts = df['Burn_Label'].value_counts().rename(index={1: 'Burn', 0: 'Unburn'})\n",
    "\n",
    "# Display the counts with labels\n",
    "print(burn_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_count = burn_counts['Burn']\n",
    "unburn_sample = df[df['Burn_Label'] == 0].sample(n=burn_count, random_state=42)\n",
    "\n",
    "downsampled_df = pd.concat([df[df['Burn_Label'] == 1], unburn_sample])\n",
    "\n",
    "# Check Burn Records\n",
    "burn_counts = downsampled_df['Burn_Label'].value_counts().rename(index={1: 'Burn', 0: 'Unburn'})\n",
    "\n",
    "# Display the counts with labels\n",
    "print(burn_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove infinite values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing infinite with nan \n",
    "downsampled_df.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "  \n",
    "# Dropping all the rows with nan values \n",
    "downsampled_df.dropna(inplace=True)\n",
    "\n",
    "# Printing df \n",
    "display(downsampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate Burn_Label from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Burn_Label from DataFrame\n",
    "burn_label = downsampled_df[['Burn_Label']]\n",
    "\n",
    "# Drop Label from DataFrame\n",
    "downsampled_df = downsampled_df.drop(columns=['Burn_Label'])\n",
    "\n",
    "# Change type of Label to Integer Format\n",
    "burn_label = burn_label.astype('int32')\n",
    "display(burn_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Data with MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the dataframe with a list of the columns\n",
    "cols_norm = downsampled_df.columns.tolist()\n",
    "\n",
    "# Import Normalize technique\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize data\n",
    "scaler.fit(downsampled_df)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_save_path = r'Export Model'\n",
    "save_path = os.path.join(scaler_save_path, 'MinMax_Scaler_DownSP.pkl')\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Normalize Data\n",
    "df_norm = scaler.transform(downsampled_df)\n",
    "df_norm = pd.DataFrame(df_norm, columns=cols_norm)\n",
    "\n",
    "# Check df_norm shape after normalization\n",
    "print(\"Shape of df_norm after normalization:\", df_norm.shape)\n",
    "\n",
    "# Concatenate df_norm with burn_label\n",
    "df_norm = pd.concat([df_norm.reset_index(drop=True), burn_label.reset_index(drop=True)], axis=1, sort=False)\n",
    "display(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_gbm(lgbm_learning_rate, num_leaves, df_norm):\n",
    "    # Define the features and target\n",
    "    X = df_norm.drop(columns=['Burn_Label'])\n",
    "    Y = df_norm['Burn_Label']\n",
    "    \n",
    "    max_depth_range = range(2, 11)  # Range of max_depth to test (2 to 10)\n",
    "    cv_scores = []  # Store mean cross-validation scores\n",
    "    cv_std_devs = []  # Store standard deviation of cross-validation scores\n",
    "    \n",
    "    print(\"Performing hyperparameter tuning for max_depth in range 2-10...\")\n",
    "    \n",
    "    for max_depth in max_depth_range:\n",
    "        params = {\n",
    "            'verbose': -1,\n",
    "            'learning_rate': lgbm_learning_rate,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'max_depth': max_depth,\n",
    "            'num_leaves': num_leaves,\n",
    "            'device': 'gpu'  # Use GPU for computation\n",
    "        }\n",
    "        \n",
    "        # Initialize the LightGBM model\n",
    "        lgbm_model = LGBMClassifier(**params)\n",
    "        \n",
    "        # 10-fold cross-validation\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        lgbm_scores = cross_val_score(lgbm_model, X, Y, cv=kf)\n",
    "        \n",
    "        # Store the mean and std of cross-validation scores\n",
    "        cv_scores.append(lgbm_scores.mean())\n",
    "        cv_std_devs.append(lgbm_scores.std())\n",
    "        \n",
    "        print(f\"Max Depth: {max_depth}, CV Score: {round(lgbm_scores.mean() * 100, 2)}%, Std Dev: {round(lgbm_scores.std() * 100, 2)}%\")\n",
    "    \n",
    "    # Find the optimal max_depth based on highest mean CV score\n",
    "    optimal_index = np.argmax(cv_scores)\n",
    "    optimal_max_depth = max_depth_range[optimal_index]\n",
    "    best_cv_score = cv_scores[optimal_index]\n",
    "    \n",
    "    print(f\"\\nOptimal Max Depth: {optimal_max_depth} with CV Score: {round(best_cv_score * 100, 2)}%\")\n",
    "    \n",
    "    # Plot the cross-validation results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(max_depth_range, cv_scores, marker='o', linestyle='-', color='b', label='Mean CV Score')\n",
    "    plt.fill_between(max_depth_range, \n",
    "                     np.array(cv_scores) - np.array(cv_std_devs), \n",
    "                     np.array(cv_scores) + np.array(cv_std_devs), \n",
    "                     color='lightblue', alpha=0.5, label='Std Dev')\n",
    "    plt.title('Hyperparameter Tuning: Max Depth vs CV Score')\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Mean CV Score')\n",
    "    plt.xticks(max_depth_range)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Fit the model with the optimal max_depth\n",
    "    print(f\"Fitting the final model with optimal max_depth = {optimal_max_depth}...\")\n",
    "    final_params = {\n",
    "        'verbose': -1,\n",
    "        'learning_rate': lgbm_learning_rate,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': optimal_max_depth,\n",
    "        'num_leaves': num_leaves,\n",
    "    }\n",
    "    \n",
    "    lgbm_model = LGBMClassifier(**final_params)\n",
    "    lgbm_model.fit(X, Y)\n",
    "    \n",
    "    print(\"\\nModel fitting complete.\")\n",
    "    \n",
    "    # Generate classification report and confusion matrix\n",
    "    print(\"\\nGenerating classification report and confusion matrix...\")\n",
    "    y_pred = cross_val_predict(lgbm_model, X, Y, cv=kf)\n",
    "    report = classification_report(Y, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(Y, y_pred)\n",
    "    \n",
    "    # Create a summary of the results\n",
    "    lgbm_result = [{\n",
    "        'Classifier': 'LightGBM',\n",
    "        'Model Definition': lgbm_model,\n",
    "        'Class 0 - Precision': report['0']['precision'],\n",
    "        'Class 0 - Recall': report['0']['recall'],\n",
    "        'Class 0 - F1-Score': report['0']['f1-score'],\n",
    "        'Class 1 - Precision': report['1']['precision'],\n",
    "        'Class 1 - Recall': report['1']['recall'],\n",
    "        'Class 1 - F1-Score': report['1']['f1-score'],\n",
    "        'Average - Precision': report['macro avg']['precision'],\n",
    "        'Average - Recall': report['macro avg']['recall'],\n",
    "        'Average - F1-Score': report['macro avg']['f1-score'],\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Confusion Matrix': cm\n",
    "    }]\n",
    "    \n",
    "    lgbm_result_df = pd.DataFrame(lgbm_result)\n",
    "    \n",
    "    display(Markdown(\"### Classification Report of LightGBM (Best Max Depth)\"))\n",
    "    display(lgbm_result_df)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix - LightGBM (Best Max Depth)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    return lgbm_model, optimal_max_depth, best_cv_score\n",
    "\n",
    "# Call the function with the parameters and your DataFrame\n",
    "lgbm_model, optimal_max_depth, best_cv_score = light_gbm(0.05, 31, df_norm)\n",
    "\n",
    "print(f\"Model trained with optimal max_depth: {optimal_max_depth}, achieving best CV score of: {round(best_cv_score * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export LightGBM Model as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = r'Export Model'\n",
    "lgbm_filename_model = os.path.join(savepath, 'Model_LGBM_DownSP.sav')\n",
    "pickle.dump(lgbm_model, open(lgbm_filename_model, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(df, random_forest_estimator_num=100, criterion=\"gini\"):\n",
    "    \"\"\"Performs hyperparameter tuning for Random Forest to find optimal max_depth\n",
    "    and plots the results.\"\"\"\n",
    "    display(Markdown(\"### Hyperparameter Tuning for Random Forest\"))\n",
    "    print()  # Add Blank Line\n",
    "\n",
    "    # Define the features and target\n",
    "    X = df.drop(columns=['Burn_Label'])  # Features: all columns except Burn_Label\n",
    "    Y = df['Burn_Label']                 # Target: Burn_Label column\n",
    "\n",
    "    # Set up cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)  # 10-fold cross-validation\n",
    "    \n",
    "    max_depths = list(range(2, 11))  # Range of max_depth to test\n",
    "    mean_cv_scores = []\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    for max_depth in max_depths:\n",
    "        rdf_model = RandomForestClassifier(n_estimators=random_forest_estimator_num,\n",
    "                                           max_depth=max_depth,\n",
    "                                           criterion=criterion,\n",
    "                                           random_state=42)\n",
    "        scores_cv = cross_val_score(rdf_model, X, Y, cv=kf)\n",
    "        mean_cv_scores.append(scores_cv.mean())\n",
    "\n",
    "    # Plotting the mean cross-validation accuracy vs. max_depth\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(max_depths, mean_cv_scores, marker='o', linestyle='-', color='blue')\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Mean CV Accuracy')\n",
    "    plt.title('Random Forest - Max Depth vs. Mean CV Accuracy')\n",
    "    plt.xticks(max_depths)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Find the best max_depth\n",
    "    best_max_depth = max_depths[np.argmax(mean_cv_scores)]\n",
    "    print(f\"Best Max Depth: {best_max_depth} with Mean CV Accuracy: {round(max(mean_cv_scores) * 100, 2)}%\")\n",
    "\n",
    "    # Train and evaluate Random Forest with the best max_depth\n",
    "    rdf_model_best = RandomForestClassifier(n_estimators=random_forest_estimator_num,\n",
    "                                            max_depth=best_max_depth,\n",
    "                                            criterion=criterion,\n",
    "                                            random_state=42)\n",
    "    rdf_model_best.fit(X, Y)\n",
    "    scores_cv = cross_val_score(rdf_model_best, X, Y, cv=kf)\n",
    "    mean_cv = scores_cv.mean()\n",
    "    std_cv = scores_cv.std()\n",
    "\n",
    "    # Display Cross-validation results\n",
    "    print(f\"Random Forest (Best Max Depth = {best_max_depth}) Cross-validation scores: {round(mean_cv * 100, 2)}%\")\n",
    "    print(f\"Random Forest (Best Max Depth = {best_max_depth}) Standard deviation: {round(std_cv * 100, 2)}%\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    y_pred = cross_val_predict(rdf_model_best, X, Y, cv=kf)\n",
    "    report = classification_report(Y, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(Y, y_pred)\n",
    "    \n",
    "    rdf_result = [{\n",
    "        'Classifier': 'Random Forest',\n",
    "        'Model Definition': rdf_model_best,\n",
    "        'Class 0 - Precision': report['0']['precision'],\n",
    "        'Class 0 - Recall': report['0']['recall'],\n",
    "        'Class 0 - F1-Score': report['0']['f1-score'],\n",
    "        'Class 1 - Precision': report['1']['precision'],\n",
    "        'Class 1 - Recall': report['1']['recall'],\n",
    "        'Class 1 - F1-Score': report['1']['f1-score'],\n",
    "        'Average - Precision': report['macro avg']['precision'],\n",
    "        'Average - Recall': report['macro avg']['recall'],\n",
    "        'Average - F1-Score': report['macro avg']['f1-score'],\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Confusion Matrix': cm\n",
    "    }]\n",
    "    \n",
    "    rdf_result_df = pd.DataFrame(rdf_result)\n",
    "    \n",
    "    display(Markdown(\"### Classification Report of Random Forest (Best Max Depth)\"))\n",
    "    display(rdf_result_df)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix - Random Forest (Best Max Depth)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    print()  # Add Blank Line\n",
    "    display(Markdown(\"<span style='color: green; font-weight: bold;'>Random Forest Model Run Complete</span>\"))\n",
    "    print()  # Add Blank Line\n",
    "\n",
    "    return rdf_model_best, mean_cv, std_cv\n",
    "\n",
    "# Call the tune_random_forest function with the desired parameters and your DataFrame\n",
    "rdf_model_best, mean_cv_best, std_cv_best = random_forest(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Random Forest as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = r'Export Model'\n",
    "rdf_filename_model = os.path.join(savepath, 'Model_RDF.sav')\n",
    "pickle.dump(rdf_model_best, open(rdf_filename_model, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GISTDA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
