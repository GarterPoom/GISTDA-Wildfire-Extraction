{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GISTDA Wildfire Machine Learning Training\n",
    "\n",
    "### This project is focused on developing and training machine learning models to predict and monitor wildfires. It utilizes datasets from Sentinel-2 Images as Raster GeoTIFF format which have been wildfire extraction, analyzes environmental factors, and applies machine learning algorithms to enhance prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **Numpy (`numpy`)**\n",
    "   - **Description**: A powerful library for numerical computations in Python. It provides support for arrays, matrices, and a large collection of mathematical functions to operate on these data structures, making it essential for scientific and statistical analysis.\n",
    "\n",
    "##### 2. **Pandas (`pandas`)**\n",
    "   - **Description**: A widely-used library for data manipulation and analysis. It provides data structures such as DataFrames to store and manipulate large datasets, making data cleaning, transformation, and exploration more efficient and intuitive.\n",
    "\n",
    "##### 3. **Matplotlib (`matplotlib.pyplot`)**\n",
    "   - **Description**: A plotting library that provides tools for creating a wide range of static, animated, and interactive visualizations in Python. It is commonly used for generating graphs, charts, and other visual data representations.\n",
    "\n",
    "##### 4. **Seaborn (`seaborn`)**\n",
    "   - **Description**: A statistical data visualization library built on top of Matplotlib. It offers high-level functions for creating informative and attractive visualizations, especially useful for exploring and understanding data trends and distributions.\n",
    "\n",
    "##### 5. **OS (`os`)**\n",
    "   - **Description**: A standard library in Python that provides functions to interact with the operating system, allowing you to work with directories, files, and system paths. It's useful for handling file operations, environment variables, and system commands.\n",
    "\n",
    "##### 6. **Pickle (`pickle`)**\n",
    "   - **Description**: A Python module used to serialize and deserialize Python objects, allowing you to save complex data structures to files and load them back into your program. It’s commonly used for saving trained models or intermediate data states.\n",
    "\n",
    "##### 7. **Dask (`dask.dataframe` and `dask`)**\n",
    "   - **Description**: A parallel computing library that scales up computations on larger datasets. `dask.dataframe` provides similar functionality to Pandas DataFrames but can handle larger-than-memory data by performing parallel, chunked computations.\n",
    "\n",
    "##### 8. **Rasterio (`rasterio`)**\n",
    "   - **Description**: A library for reading and writing geospatial raster data. It’s widely used for working with geospatial data in formats like GeoTIFF, allowing for operations on large image files commonly used in remote sensing and GIS applications.\n",
    "\n",
    "##### 9. **Scikit-Learn (`sklearn`)**\n",
    "   - **Description**: A robust library for machine learning that provides simple and efficient tools for data analysis and modeling. It includes modules like `MinMaxScaler` for scaling data, `cross_val_score` and `cross_val_predict` for evaluating models, and various classifiers.\n",
    "\n",
    "##### 10. **LightGBM (`lightgbm`)**\n",
    "   - **Description**: A high-performance, gradient-boosting framework developed by Microsoft. It's optimized for speed and efficiency on large datasets and is particularly well-suited for structured data and classification problems.\n",
    "\n",
    "##### 11. **IPython (`IPython.display`)**\n",
    "   - **Description**: A library for creating interactive elements in Jupyter Notebooks, such as displaying Markdown, HTML, and other rich content. It’s often used to improve the readability and interactivity of notebook outputs.\n",
    "\n",
    "##### 12. **Delayed and Compute (`dask.delayed` and `dask.compute`)**\n",
    "   - **Description**: Functions in the Dask library that allow you to parallelize and execute tasks asynchronously. `delayed` is used to mark a function for lazy evaluation, and `compute` is used to execute the delayed functions, making computations efficient and scalable.\n",
    "\n",
    "##### 13. **Rasterio Windows (`rasterio.windows`)**\n",
    "   - **Description**: A submodule in Rasterio that allows for windowed or tiled reading of raster data. This is useful for reading and processing only portions of large raster datasets, improving efficiency when working with large geospatial files.\n",
    "\n",
    "##### 14. **XGBoost (`xgboost`)**\n",
    "   - **Description**: An optimized gradient-boosting framework that’s highly effective for predictive modeling tasks, especially classification and regression. It’s known for its speed and performance and is widely used in machine learning competitions for its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import dask.dataframe as dd\n",
    "import rasterio\n",
    "import dask\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "from IPython.display import display, Markdown\n",
    "from dask import delayed, compute\n",
    "from rasterio.windows import Window\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Raster GeoTIFF files\n",
    "\n",
    "This code reads large raster files in manageable chunks, processes each chunk, and stores the data in a consolidated DataFrame.\n",
    "\n",
    "1. **Folder Path and Chunk Size**: Sets the path to the folder containing raster files (`raster_train_file_path`) and defines the chunk size (`CHUNK_SIZE`).\n",
    "\n",
    "2. **`read_raster_in_chunks` Function**:\n",
    "   - Reads a raster file chunk-by-chunk.\n",
    "   - For each chunk, it extracts data from all bands, along with the pixel coordinates.\n",
    "   - Stores each chunk's data in a temporary DataFrame, which is added to a list.\n",
    "\n",
    "3. **Task Creation**:\n",
    "   - Creates a list of tasks using Dask's `@delayed` to process each raster file in parallel.\n",
    "\n",
    "4. **Compute and Combine**:\n",
    "   - Computes each task to get individual DataFrames for each file, then combines them into a final DataFrame (`final_df`).\n",
    "\n",
    "5. **Debug Outputs**:\n",
    "   - Prints the shape, columns, and a sample of the final DataFrame for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the raster files\n",
    "raster_train_file_path = r'Raster_Train'\n",
    "\n",
    "# Parameters for chunk size\n",
    "CHUNK_SIZE = 512 # Adjust following your hardware specification\n",
    "\n",
    "@delayed\n",
    "def read_raster_in_chunks(raster_path, file, root):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        height, width = src.height, src.width\n",
    "        num_bands = src.count\n",
    "        band_names = [f'B{str(i).zfill(2)}' for i in range(1, num_bands + 1)]\n",
    "        chunk_dfs = []\n",
    "        \n",
    "        # Loop over the raster in chunks\n",
    "        for row in range(0, height, CHUNK_SIZE):\n",
    "            for col in range(0, width, CHUNK_SIZE):\n",
    "                window = Window(col_off=col, row_off=row, \n",
    "                              width=min(CHUNK_SIZE, width - col),\n",
    "                              height=min(CHUNK_SIZE, height - row))\n",
    "                \n",
    "                # Read all bands at once\n",
    "                data = src.read(window=window)\n",
    "                \n",
    "                # Check if chunk contains any data\n",
    "                if np.any(data):\n",
    "                    rows, cols = data[0].shape\n",
    "                    \n",
    "                    # Create base DataFrame with coordinates\n",
    "                    row_coords, col_coords = np.meshgrid(\n",
    "                        np.arange(row, row + rows),\n",
    "                        np.arange(col, col + cols),\n",
    "                        indexing=\"ij\"\n",
    "                    )\n",
    "                    \n",
    "                    chunk_df = pd.DataFrame({\n",
    "                        'raster_file': file,\n",
    "                        'subfolder': os.path.basename(root),\n",
    "                        'x': row_coords.flatten(),\n",
    "                        'y': col_coords.flatten()\n",
    "                    })\n",
    "                    \n",
    "                    # Add each band's data\n",
    "                    for band_idx, band_name in enumerate(band_names, 1):\n",
    "                        chunk_df[band_name] = data[band_idx-1].flatten()\n",
    "                    \n",
    "                    chunk_dfs.append(chunk_df)\n",
    "        \n",
    "        return pd.concat(chunk_dfs, ignore_index=True) if chunk_dfs else pd.DataFrame()\n",
    "\n",
    "# Create list of tasks\n",
    "dask_dfs = [\n",
    "    read_raster_in_chunks(os.path.join(root, file), file, root)\n",
    "    for root, dirs, files in os.walk(raster_train_file_path)\n",
    "    for file in files if file.endswith('.tif')\n",
    "]\n",
    "\n",
    "# Compute all tasks\n",
    "dataframes = compute(*dask_dfs)\n",
    "\n",
    "# Combine all DataFrames\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Debug prints\n",
    "print(\"DataFrame shape:\", final_df.shape)\n",
    "print(\"\\nDataFrame columns:\", final_df.columns.tolist())\n",
    "print(\"\\nSample of data:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) & Feature Engineering\n",
    "\n",
    "This code performs several operations on a DataFrame (`final_df`), converting it to a Dask DataFrame for distributed processing, renaming columns, and dropping unnecessary columns:\n",
    "\n",
    "1. **Convert to Dask DataFrame**:\n",
    "   - Converts the existing Pandas DataFrame (`final_df`) into a Dask DataFrame (`ddf`) to enable parallel, distributed processing.\n",
    "\n",
    "2. **Rename Columns**:\n",
    "   - Defines a list of new column names for Sentinel-2 bands and other data (`new_col_names`).\n",
    "   - Renames `ddf` columns using this list to make them more descriptive.\n",
    "\n",
    "3. **Drop Unneeded Columns**:\n",
    "   - Drops columns such as `raster_file`, `subfolder`, `x`, `y`, and `dNBR`, keeping only essential information in the DataFrame (`df`).\n",
    "\n",
    "4. **Display DataFrame**:\n",
    "   - Uses `display()` to view the DataFrame when needed, triggering Dask's computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "new_col_names = ['raster_file', 'subfolder', 'x', 'y', 'Band_2', 'Band_3', 'Band_4', \n",
    "                 'Band_5', 'Band_6', 'Band_7', 'Band_8', 'Band_8A', 'Band_11', \n",
    "                 'Band_12', 'dNBR', 'NDVI', 'NDWI', 'Burn_Label']\n",
    "df = final_df.rename(columns=dict(zip(final_df.columns, new_col_names)))\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['raster_file', 'subfolder', 'x', 'y', 'dNBR']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display DataFrame (computing results)\n",
    "print(df.head())  # Displays a small, computed sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Burn Class\n",
    "\n",
    "This code checks and displays the counts of burn records in the DataFrame:\n",
    "\n",
    "1. **Count Burn Labels**:\n",
    "   - Counts occurrences in the `Burn_Label` column to determine the number of \"Burn\" and \"Unburn\" records.\n",
    "   - Renames the labels: `1` to \"Burn\" and `0` to \"Unburn\" for readability.\n",
    "\n",
    "2. **Display Counts**:\n",
    "   - Prints the resulting counts to show the distribution of burn and unburned areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Burn Records\n",
    "burn_counts = df['Burn_Label'].value_counts().rename(index={1: 'Burn', 0: 'Unburn'})\n",
    "\n",
    "# Display the counts with labels\n",
    "print(burn_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "\n",
    "This code performs downsampling to balance the dataset by reducing the number of \"Unburn\" records to match the count of \"Burn\" records:\n",
    "\n",
    "1. **Get Burn Count**:\n",
    "   - Retrieves the count of \"Burn\" records from `burn_counts`.\n",
    "\n",
    "2. **Sample Unburned Records**:\n",
    "   - Selects a random sample of \"Unburn\" records, equal in size to the number of \"Burn\" records, using a fixed `random_state` for reproducibility.\n",
    "\n",
    "3. **Combine Burn and Downsampled Unburn Records**:\n",
    "   - Combines all \"Burn\" records with the downsampled \"Unburn\" sample into a new DataFrame (`downsampled_df`).\n",
    "\n",
    "4. **Check New Burn Record Counts**:\n",
    "   - Counts and displays the \"Burn\" and \"Unburn\" records in `downsampled_df` to verify balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_count = burn_counts['Burn']\n",
    "unburn_sample = df[df['Burn_Label'] == 0].sample(n=burn_count, random_state=42)\n",
    "\n",
    "downsampled_df = pd.concat([df[df['Burn_Label'] == 1], unburn_sample])\n",
    "\n",
    "# Check Burn Records\n",
    "burn_counts = downsampled_df['Burn_Label'].value_counts().rename(index={1: 'Burn', 0: 'Unburn'})\n",
    "\n",
    "# Display the counts with labels\n",
    "print(burn_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove infinite values\n",
    "\n",
    "This code handles the presence of infinite values and missing data in the `downsampled_df` DataFrame:\n",
    "\n",
    "1. **Replace Infinite Values**:\n",
    "   - Replaces both positive and negative infinite values (`np.inf` and `-np.inf`) with `NaN` using `replace()`. This ensures that infinite values do not interfere with further processing.\n",
    "\n",
    "2. **Drop Rows with Missing Values**:\n",
    "   - Removes any rows containing `NaN` values using `dropna()`, ensuring the DataFrame only contains valid data.\n",
    "\n",
    "3. **Display the DataFrame**:\n",
    "   - Displays the cleaned DataFrame (`downsampled_df`) for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing infinite with nan \n",
    "downsampled_df.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "  \n",
    "# Dropping all the rows with nan values \n",
    "downsampled_df.dropna(inplace=True)\n",
    "\n",
    "# Printing df \n",
    "display(downsampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate Burn_Label from DataFrame\n",
    "\n",
    "This code separates the `Burn_Label` from the main DataFrame and ensures the label is in the correct format:\n",
    "\n",
    "1. **Separate Burn Label**:\n",
    "   - Extracts the `Burn_Label` column from `downsampled_df` into a new DataFrame (`burn_label`).\n",
    "\n",
    "2. **Remove Burn Label from Main DataFrame**:\n",
    "   - Drops the `Burn_Label` column from `downsampled_df` to ensure only the feature data remains.\n",
    "\n",
    "3. **Convert Burn Label to Integer**:\n",
    "   - Changes the data type of the `burn_label` DataFrame to `int32` to ensure consistent and efficient processing.\n",
    "\n",
    "4. **Display Burn Label**:\n",
    "   - Displays the modified `burn_label` DataFrame to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Burn_Label from DataFrame\n",
    "burn_label = downsampled_df[['Burn_Label']]\n",
    "\n",
    "# Drop Label from DataFrame\n",
    "downsampled_df = downsampled_df.drop(columns=['Burn_Label'])\n",
    "\n",
    "# Change type of Label to Integer Format\n",
    "burn_label = burn_label.astype('int32')\n",
    "display(burn_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Data with MinMax Scaler\n",
    "\n",
    "This code normalizes the feature data and saves the normalization model, while also combining it with the `Burn_Label`:\n",
    "\n",
    "1. **List Columns**:\n",
    "   - Creates a list of column names from `downsampled_df` to keep track of the column order after normalization (`cols_norm`).\n",
    "\n",
    "2. **Normalize the Data**:\n",
    "   - Imports `MinMaxScaler` from `sklearn` and fits it to the data in `downsampled_df`, which scales the values between 0 and 1.\n",
    "\n",
    "3. **Save the Scaler**:\n",
    "   - Saves the fitted `MinMaxScaler` model to a specified path (`MinMax_Scaler.pkl`) for later use.\n",
    "\n",
    "4. **Apply Normalization**:\n",
    "   - Normalizes the data by applying the `scaler` to `downsampled_df`, then converts the result back into a DataFrame (`df_norm`) with the original column names.\n",
    "\n",
    "5. **Check Shape**:\n",
    "   - Prints the shape of `df_norm` to confirm the normalization was applied correctly.\n",
    "\n",
    "6. **Concatenate with Burn Label**:\n",
    "   - Combines the normalized feature data (`df_norm`) with the `burn_label` DataFrame, aligning them by their indices and ensuring the result is a complete dataset.\n",
    "\n",
    "7. **Display the DataFrame**:\n",
    "   - Displays the final DataFrame (`df_norm`), which now includes both the normalized features and the `Burn_Label`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the dataframe with a list of the columns\n",
    "cols_norm = downsampled_df.columns.tolist()\n",
    "\n",
    "# Import Normalize technique\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize data\n",
    "scaler.fit(downsampled_df)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_save_path = r'Export_Model'\n",
    "save_path = os.path.join(scaler_save_path, 'MinMax_Scaler.pkl')\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Normalize Data\n",
    "df_norm = scaler.transform(downsampled_df)\n",
    "df_norm = pd.DataFrame(df_norm, columns=cols_norm)\n",
    "\n",
    "# Check df_norm shape after normalization\n",
    "print(\"Shape of df_norm after normalization:\", df_norm.shape)\n",
    "\n",
    "# Concatenate df_norm with burn_label\n",
    "df_norm = pd.concat([df_norm.reset_index(drop=True), burn_label.reset_index(drop=True)], axis=1, sort=False)\n",
    "display(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models Training Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Model Training\n",
    "\n",
    "#### LightGBM Hyperparameter Tuning and Model Evaluation\n",
    "\n",
    "The function `light_gbm` is designed to perform hyperparameter tuning for a LightGBM model by testing different values of `max_depth` and evaluating the performance using cross-validation. Here's a breakdown of the code:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `lgbm_learning_rate`: The learning rate for the LightGBM model.\n",
    "   - `num_leaves`: The number of leaves used in the model.\n",
    "   - `df_norm`: A DataFrame containing the dataset, where the target variable is named `Burn_Label`.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - The target variable `Burn_Label` is separated from the features (`X`) and the target (`Y`).\n",
    "   \n",
    "3. **Hyperparameter Tuning**:\n",
    "   - The function tests different values of `max_depth` in the range 2 to 10 using cross-validation.\n",
    "   - For each value of `max_depth`, a LightGBM classifier is created with the provided learning rate and number of leaves, and 10-fold cross-validation is performed to evaluate the model’s performance.\n",
    "   - The mean and standard deviation of the cross-validation scores for each `max_depth` are stored and displayed.\n",
    "\n",
    "4. **Optimal Hyperparameter Selection**:\n",
    "   - The function identifies the optimal value of `max_depth` based on the highest mean cross-validation score.\n",
    "   \n",
    "5. **Model Fitting**:\n",
    "   - The model is then trained using the optimal `max_depth` and the specified parameters.\n",
    "   - A classification report and confusion matrix are generated for model evaluation.\n",
    "\n",
    "6. **Visualization**:\n",
    "   - The results of the hyperparameter tuning (mean cross-validation score and standard deviation) are plotted against the `max_depth`.\n",
    "   - A confusion matrix heatmap is also generated to visualize the performance of the model.\n",
    "\n",
    "7. **Output**:\n",
    "   - The function returns the trained LightGBM model, the optimal `max_depth`, and the best cross-validation score.\n",
    "   \n",
    "8. **Example Usage**:\n",
    "   - The function is called with the learning rate `0.05`, `num_leaves` set to `31`, and a DataFrame `df_norm` containing the dataset.\n",
    "\n",
    "The function helps in tuning the `max_depth` parameter of the LightGBM model, training it with the optimal value, and generating a detailed evaluation report and confusion matrix for performance assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_gbm(lgbm_learning_rate, num_leaves, df_norm):\n",
    "    # Define the features and target\n",
    "    X = df_norm.drop(columns=['Burn_Label'])\n",
    "    Y = df_norm['Burn_Label']\n",
    "    \n",
    "    max_depth_range = range(2, 21)  # Range of max_depth to test (2 to 20)\n",
    "    cv_scores = []  # Store mean cross-validation scores\n",
    "    cv_std_devs = []  # Store standard deviation of cross-validation scores\n",
    "    \n",
    "    print(\"Performing hyperparameter tuning for max_depth in range 2-20...\")\n",
    "    \n",
    "    for max_depth in max_depth_range:\n",
    "        params = {\n",
    "            'verbose': -1,\n",
    "            'learning_rate': lgbm_learning_rate,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'max_depth': max_depth,\n",
    "            'num_leaves': num_leaves\n",
    "        }\n",
    "        \n",
    "        # Initialize the LightGBM model\n",
    "        lgbm_model = LGBMClassifier(**params)\n",
    "        \n",
    "        # 10-fold cross-validation\n",
    "        kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        lgbm_scores = cross_val_score(lgbm_model, X, Y, cv=kf)\n",
    "        \n",
    "        # Store the mean and std of cross-validation scores\n",
    "        cv_scores.append(lgbm_scores.mean())\n",
    "        cv_std_devs.append(lgbm_scores.std())\n",
    "        \n",
    "        print(f\"Max Depth: {max_depth}, CV Score: {round(lgbm_scores.mean() * 100, 2)}%, Std Dev: {round(lgbm_scores.std() * 100, 2)}%\")\n",
    "    \n",
    "    # Find the optimal max_depth based on highest mean CV score\n",
    "    optimal_index = np.argmax(cv_scores)\n",
    "    optimal_max_depth = max_depth_range[optimal_index]\n",
    "    best_cv_score = cv_scores[optimal_index]\n",
    "    \n",
    "    print(f\"\\nOptimal Max Depth: {optimal_max_depth} with CV Score: {round(best_cv_score * 100, 2)}%\")\n",
    "    \n",
    "    # Plot the cross-validation results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(max_depth_range, cv_scores, marker='o', linestyle='-', color='b', label='Mean CV Score')\n",
    "    plt.fill_between(max_depth_range, \n",
    "                     np.array(cv_scores) - np.array(cv_std_devs), \n",
    "                     np.array(cv_scores) + np.array(cv_std_devs), \n",
    "                     color='lightblue', alpha=0.5, label='Std Dev')\n",
    "    plt.title('Hyperparameter Tuning: Max Depth vs CV Score')\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Mean CV Score')\n",
    "    plt.xticks(max_depth_range)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Fit the model with the optimal max_depth\n",
    "    print(f\"Fitting the final model with optimal max_depth = {optimal_max_depth}...\")\n",
    "    final_params = {\n",
    "        'verbose': -1,\n",
    "        'learning_rate': lgbm_learning_rate,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': optimal_max_depth,\n",
    "        'num_leaves': num_leaves\n",
    "    }\n",
    "    \n",
    "    lgbm_model = LGBMClassifier(**final_params)\n",
    "    lgbm_model.fit(X, Y)\n",
    "    \n",
    "    print(\"\\nModel fitting complete.\")\n",
    "    \n",
    "    # Generate classification report and confusion matrix\n",
    "    print(\"\\nGenerating classification report and confusion matrix...\")\n",
    "    y_pred = cross_val_predict(lgbm_model, X, Y, cv=kf)\n",
    "    report = classification_report(Y, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(Y, y_pred)\n",
    "    \n",
    "    # Create a summary of the results\n",
    "    lgbm_result = [{\n",
    "        'Classifier': 'LightGBM',\n",
    "        'Model Definition': lgbm_model,\n",
    "        'Class 0 - Precision': report['0']['precision'],\n",
    "        'Class 0 - Recall': report['0']['recall'],\n",
    "        'Class 0 - F1-Score': report['0']['f1-score'],\n",
    "        'Class 1 - Precision': report['1']['precision'],\n",
    "        'Class 1 - Recall': report['1']['recall'],\n",
    "        'Class 1 - F1-Score': report['1']['f1-score'],\n",
    "        'Average - Precision': report['macro avg']['precision'],\n",
    "        'Average - Recall': report['macro avg']['recall'],\n",
    "        'Average - F1-Score': report['macro avg']['f1-score'],\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Confusion Matrix': cm\n",
    "    }]\n",
    "    \n",
    "    lgbm_result_df = pd.DataFrame(lgbm_result)\n",
    "    \n",
    "    display(Markdown(\"### Classification Report of LightGBM (Best Max Depth)\"))\n",
    "    display(lgbm_result_df)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix - LightGBM (Best Max Depth)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    return lgbm_model, optimal_max_depth, best_cv_score\n",
    "\n",
    "# Call the function with the parameters and your DataFrame\n",
    "lgbm_model, optimal_max_depth, best_cv_score = light_gbm(0.05, 31, df_norm)\n",
    "\n",
    "print(f\"Model trained with optimal max_depth: {optimal_max_depth}, achieving best CV score of: {round(best_cv_score * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export LightGBM Model as pickle\n",
    "\n",
    "After training the LightGBM model, the following code is used to save the trained model to disk for future use:\n",
    "\n",
    "1. **Define Save Path**:\n",
    "   - `savepath = r'Export_Model'`: This line sets the directory path where the trained model will be saved. The path is a string and points to a folder named `Export_Model`.\n",
    "\n",
    "2. **Create File Path for the Model**:\n",
    "   - `lgbm_filename_model = os.path.join(savepath, 'Model_LGBM.sav')`: This line creates the full path to save the model. It combines the `savepath` directory with the filename `Model_LGBM.sav`.\n",
    "\n",
    "3. **Save the Model Using Pickle**:\n",
    "   - `pickle.dump(lgbm_model, open(lgbm_filename_model, 'wb'))`: The `pickle.dump()` function is used to serialize the trained LightGBM model (`lgbm_model`) and save it to the file `Model_LGBM.sav`. The model is saved in binary mode (`'wb'`) to ensure that it can be loaded later for inference or further analysis.\n",
    "\n",
    "This process allows you to persist the trained model and easily load it later without having to retrain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = r'Export_Model'\n",
    "lgbm_filename_model = os.path.join(savepath, 'Model_LGBM.sav')\n",
    "pickle.dump(lgbm_model, open(lgbm_filename_model, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model Training\n",
    "\n",
    "The `xgboost_model` function performs hyperparameter tuning for the XGBoost model to find the optimal `max_depth` and evaluate its performance using cross-validation. GPU processing is enabled to speed up the training process. Below is a breakdown of the steps involved:\n",
    "\n",
    "1. **Input Parameters**:\n",
    "   - `df`: The DataFrame containing the dataset with the target variable `Burn_Label`.\n",
    "   - `xgb_estimator_num`: The number of estimators (trees) for the XGBoost model, default value is 100.\n",
    "   - `objective`: The objective function for the model, default value is `\"binary:logistic\"` for binary classification.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - The target variable (`Burn_Label`) is separated from the features, and the feature matrix (`X`) and target vector (`Y`) are defined.\n",
    "\n",
    "3. **Cross-Validation Setup**:\n",
    "   - A 10-fold cross-validation strategy (`KFold`) is used to split the data into training and validation sets for robust evaluation of the model.\n",
    "\n",
    "4. **Hyperparameter Tuning**:\n",
    "   - The function iterates through different values of `max_depth` (ranging from 2 to 20), training an XGBoost model with each `max_depth` value.\n",
    "   - For each value of `max_depth`, the model is evaluated using cross-validation, and the mean cross-validation accuracy is stored.\n",
    "\n",
    "5. **GPU Acceleration**:\n",
    "   - GPU processing is enabled by using the `tree_method=\"gpu_hist\"` and `predictor=\"gpu_predictor\"` parameters for both training and prediction. This significantly accelerates the model training and prediction times.\n",
    "\n",
    "6. **Plotting Results**:\n",
    "   - A plot is generated to visualize the relationship between `max_depth` and mean cross-validation accuracy. This helps in identifying the optimal `max_depth` for the best model.\n",
    "\n",
    "7. **Optimal Model Training**:\n",
    "   - Once the best `max_depth` is identified, the final model is trained with the optimal hyperparameters, and cross-validation is performed again to calculate the mean and standard deviation of the accuracy.\n",
    "\n",
    "8. **Model Evaluation**:\n",
    "   - A classification report and confusion matrix are generated to evaluate the model's performance in terms of precision, recall, F1-score, and overall accuracy.\n",
    "   - A confusion matrix heatmap is plotted to provide a clear visualization of the model's predictions versus the actual outcomes.\n",
    "\n",
    "9. **Output**:\n",
    "   - The function returns the best-trained XGBoost model (`xgb_model_best`), along with the mean and standard deviation of the cross-validation scores.\n",
    "\n",
    "10. **Example Usage**:\n",
    "   - The function is called with a dataset (`df_norm`), and it performs hyperparameter tuning and evaluation of the XGBoost model using GPU acceleration.\n",
    "\n",
    "This approach leverages GPU resources for faster training while optimizing model performance based on cross-validation results, making it suitable for large datasets and efficient model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_model(df_norm, xgb_estimator_num=100, objective=\"binary:logistic\"):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning for XGBoost to find optimal max_depth\n",
    "    and plots the results with GPU or CPU processing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_norm : pandas.DataFrame\n",
    "        Input dataframe containing features and target variable\n",
    "    xgb_estimator_num : int, optional (default=100)\n",
    "        Number of estimators for XGBoost\n",
    "    objective : str, optional (default=\"binary:logistic\")\n",
    "        Objective function for XGBoost\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (best_xgb_model, mean_cross_val_score, std_cross_val_score)\n",
    "    \"\"\"\n",
    "    # Check if GPU is available\n",
    "    def is_gpu_available():\n",
    "        try:\n",
    "            test_model = XGBClassifier(tree_method=\"gpu_hist\")\n",
    "            return \"gpu_hist\" in test_model.get_xgb_params().get(\"tree_method\", \"\")\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    gpu_available = is_gpu_available()\n",
    "    if gpu_available:\n",
    "        print(f\"GPU available: {gpu_available}\")\n",
    "    else:\n",
    "        print(f\"GPU unavailable, CPU Enabled.\")\n",
    "\n",
    "    display(Markdown(\"### Hyperparameter Tuning for XGBoost\"))\n",
    "    print()  # Add Blank Line\n",
    "\n",
    "    # Define the features and target\n",
    "    X = df_norm.drop(columns=['Burn_Label'])  # Features: all columns except Burn_Label\n",
    "    Y = df_norm['Burn_Label']                 # Target: Burn_Label column\n",
    "\n",
    "    # Set up cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)  # 10-fold cross-validation\n",
    "    \n",
    "    max_depths = list(range(2, 21))  # Range of max_depth to test\n",
    "    mean_cv_scores = []\n",
    "    cv_std_devs = []\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    for max_depth in max_depths:\n",
    "        xgb_model = XGBClassifier(\n",
    "            n_estimators=xgb_estimator_num,\n",
    "            max_depth=max_depth,\n",
    "            objective=objective,\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42,\n",
    "            tree_method=\"hist\",  # Use \"hist\" for both CPU and GPU\n",
    "            device=\"cuda\" if gpu_available else \"cpu\"  # Explicitly set device\n",
    "        )\n",
    "        try:\n",
    "            scores_cv = cross_val_score(xgb_model, X, Y, cv=kf, scoring='accuracy')\n",
    "            mean_cv_scores.append(scores_cv.mean())\n",
    "            cv_std_devs.append(scores_cv.std())\n",
    "            print(f\"Max Depth: {max_depth}, CV Score: {round(scores_cv.mean() * 100, 2)}%, Std Dev: {round(scores_cv.std() * 100, 2)}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing max_depth {max_depth}: {e}\")\n",
    "            mean_cv_scores.append(0)\n",
    "            \n",
    "    # Find the best max_depth\n",
    "    best_max_depth = max_depths[np.argmax(mean_cv_scores)]\n",
    "    best_accuracy = max(mean_cv_scores)\n",
    "    print(f\"Best Max Depth: {best_max_depth} with Mean CV Accuracy: {round(best_accuracy * 100, 2)}%\")\n",
    "\n",
    "    # Plotting the mean cross-validation accuracy vs. max_depth\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(max_depths, mean_cv_scores, marker='o', linestyle='-', color='blue')\n",
    "    plt.fill_between(\n",
    "        max_depths, \n",
    "        np.array(mean_cv_scores) - np.array(cv_std_devs), \n",
    "        np.array(mean_cv_scores) + np.array(cv_std_devs), \n",
    "        color='lightblue', alpha=0.5, label='Std Dev'\n",
    "    )\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Mean CV Accuracy')\n",
    "    plt.title('XGBoost - Max Depth vs. Mean CV Accuracy')\n",
    "    plt.xticks(max_depths)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Train and evaluate XGBoost with the best max_depth\n",
    "    xgb_model_best = XGBClassifier(\n",
    "        n_estimators=xgb_estimator_num,\n",
    "        max_depth=best_max_depth,\n",
    "        objective=objective,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",  # Use \"hist\" for both CPU and GPU\n",
    "        device=\"cuda\" if gpu_available else \"cpu\"  # Explicitly set device\n",
    "    )\n",
    "    xgb_model_best.fit(X, Y)\n",
    "    scores_cv = cross_val_score(xgb_model_best, X, Y, cv=kf, scoring='accuracy')\n",
    "    mean_cv = scores_cv.mean()\n",
    "    std_cv = scores_cv.std()\n",
    "\n",
    "    # Display Cross-validation results\n",
    "    print(f\"XGBoost (Best Max Depth = {best_max_depth}) Cross-validation scores: {round(mean_cv * 100, 2)}%\")\n",
    "    print(f\"XGBoost (Best Max Depth = {best_max_depth}) Standard deviation: {round(std_cv * 100, 2)}%\")\n",
    "    \n",
    "    # Generate classification report\n",
    "    y_pred = cross_val_predict(xgb_model_best, X, Y, cv=kf)\n",
    "    report = classification_report(Y, y_pred, output_dict=True)\n",
    "    cm = confusion_matrix(Y, y_pred)\n",
    "    \n",
    "    xgb_result = [{\n",
    "        'Classifier': 'XGBoost',\n",
    "        'Model Definition': xgb_model_best,\n",
    "        'Class 0 - Precision': report['0']['precision'],\n",
    "        'Class 0 - Recall': report['0']['recall'],\n",
    "        'Class 0 - F1-Score': report['0']['f1-score'],\n",
    "        'Class 1 - Precision': report['1']['precision'],\n",
    "        'Class 1 - Recall': report['1']['recall'],\n",
    "        'Class 1 - F1-Score': report['1']['f1-score'],\n",
    "        'Average - Precision': report['macro avg']['precision'],\n",
    "        'Average - Recall': report['macro avg']['recall'],\n",
    "        'Average - F1-Score': report['macro avg']['f1-score'],\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Confusion Matrix': cm\n",
    "    }]\n",
    "    \n",
    "    xgb_result_df = pd.DataFrame(xgb_result)\n",
    "    \n",
    "    display(Markdown(\"### Classification Report of XGBoost (Best Max Depth)\"))\n",
    "    display(xgb_result_df)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix - XGBoost (Best Max Depth)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    print()  # Add Blank Line\n",
    "    display(Markdown(\"<span style='color: green; font-weight: bold;'>XGBoost Model Run Complete (GPU/CPU Enabled)</span>\"))\n",
    "    print()  # Add Blank Line\n",
    "\n",
    "    return xgb_model_best, mean_cv, std_cv\n",
    "\n",
    "# Call the xgboost_model function with the desired parameters and your DataFrame\n",
    "xgb_model_best, mean_cv_best, std_cv_best = xgboost_model(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export XGBoost as pickle\n",
    "\n",
    "After training the XGBoost model with the optimal hyperparameters, the model is saved to a file for later use. The following steps are involved in saving the trained model:\n",
    "\n",
    "1. **Define Save Path**:\n",
    "   - The variable `savepath` is set to a directory called `'Export_Model'`, where the model will be saved. If the directory does not exist, it should be created manually or via additional code.\n",
    "\n",
    "2. **Create the File Path**:\n",
    "   - `xgb_filename_model` constructs the full file path for saving the model. The model file will be named `Model_XGB.sav` and will be stored in the `Export_Model` directory.\n",
    "\n",
    "3. **Save the Model**:\n",
    "   - The `pickle.dump` function is used to serialize and save the trained XGBoost model (`xgb_model_best`) to the specified file path. The file is saved in binary format using the `wb` mode.\n",
    "\n",
    "4. **Usage**:\n",
    "   - This allows the trained model to be loaded later for predictions or further analysis without needing to retrain it.\n",
    "\n",
    "By saving the model, you can easily load it in the future for inference or to integrate it into a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = r'Export_Model'\n",
    "xgb_filename_model = os.path.join(savepath, 'Model_XGB.sav')\n",
    "pickle.dump(xgb_model_best, open(xgb_filename_model, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RIDA_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
