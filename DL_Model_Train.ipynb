{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GISTDA Wildfire Machine Learning Training\n",
    "\n",
    "### This project is focused on developing and training machine learning models to predict and monitor wildfires. It utilizes datasets from Sentinel-2 Images as Raster GeoTIFF format which have been wildfire extraction, analyzes environmental factors, and applies machine learning algorithms to enhance prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **Numpy (`numpy`)**\n",
    "   - **Description**: A powerful library for numerical computations in Python. It provides support for arrays, matrices, and a large collection of mathematical functions to operate on these data structures, making it essential for scientific and statistical analysis.\n",
    "\n",
    "##### 2. **Pandas (`pandas`)**\n",
    "   - **Description**: A widely-used library for data manipulation and analysis. It provides data structures such as DataFrames to store and manipulate large datasets, making data cleaning, transformation, and exploration more efficient and intuitive.\n",
    "\n",
    "##### 3. **Matplotlib (`matplotlib.pyplot`)**\n",
    "   - **Description**: A plotting library that provides tools for creating a wide range of static, animated, and interactive visualizations in Python. It is commonly used for generating graphs, charts, and other visual data representations.\n",
    "\n",
    "##### 4. **Seaborn (`seaborn`)**\n",
    "   - **Description**: A statistical data visualization library built on top of Matplotlib. It offers high-level functions for creating informative and attractive visualizations, especially useful for exploring and understanding data trends and distributions.\n",
    "\n",
    "##### 5. **OS (`os`)**\n",
    "   - **Description**: A standard library in Python that provides functions to interact with the operating system, allowing you to work with directories, files, and system paths. It's useful for handling file operations, environment variables, and system commands.\n",
    "\n",
    "##### 6. **Pickle (`pickle`)**\n",
    "   - **Description**: A Python module used to serialize and deserialize Python objects, allowing you to save complex data structures to files and load them back into your program. It’s commonly used for saving trained models or intermediate data states.\n",
    "\n",
    "##### 7. **Dask (`dask.dataframe` and `dask`)**\n",
    "   - **Description**: A parallel computing library that scales up computations on larger datasets. `dask.dataframe` provides similar functionality to Pandas DataFrames but can handle larger-than-memory data by performing parallel, chunked computations.\n",
    "\n",
    "##### 8. **Rasterio (`rasterio`)**\n",
    "   - **Description**: A library for reading and writing geospatial raster data. It’s widely used for working with geospatial data in formats like GeoTIFF, allowing for operations on large image files commonly used in remote sensing and GIS applications.\n",
    "\n",
    "##### 9. **Scikit-Learn (`sklearn`)**\n",
    "   - **Description**: A robust library for machine learning that provides simple and efficient tools for data analysis and modeling. It includes modules like `MinMaxScaler` for scaling data, `cross_val_score` and `cross_val_predict` for evaluating models, and various classifiers.\n",
    "\n",
    "##### 10. **LightGBM (`lightgbm`)**\n",
    "   - **Description**: A high-performance, gradient-boosting framework developed by Microsoft. It's optimized for speed and efficiency on large datasets and is particularly well-suited for structured data and classification problems.\n",
    "\n",
    "##### 11. **IPython (`IPython.display`)**\n",
    "   - **Description**: A library for creating interactive elements in Jupyter Notebooks, such as displaying Markdown, HTML, and other rich content. It’s often used to improve the readability and interactivity of notebook outputs.\n",
    "\n",
    "##### 12. **Delayed and Compute (`dask.delayed` and `dask.compute`)**\n",
    "   - **Description**: Functions in the Dask library that allow you to parallelize and execute tasks asynchronously. `delayed` is used to mark a function for lazy evaluation, and `compute` is used to execute the delayed functions, making computations efficient and scalable.\n",
    "\n",
    "##### 13. **Rasterio Windows (`rasterio.windows`)**\n",
    "   - **Description**: A submodule in Rasterio that allows for windowed or tiled reading of raster data. This is useful for reading and processing only portions of large raster datasets, improving efficiency when working with large geospatial files.\n",
    "\n",
    "##### 14. **XGBoost (`xgboost`)**\n",
    "   - **Description**: An optimized gradient-boosting framework that’s highly effective for predictive modeling tasks, especially classification and regression. It’s known for its speed and performance and is widely used in machine learning competitions for its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import dask.dataframe as dd\n",
    "import rasterio\n",
    "import dask\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from IPython.display import display, Markdown\n",
    "from dask import delayed, compute\n",
    "from rasterio.windows import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Raster GeoTIFF files\n",
    "\n",
    "This code reads large raster files in manageable chunks, processes each chunk, and stores the data in a consolidated DataFrame.\n",
    "\n",
    "1. **Folder Path and Chunk Size**: Sets the path to the folder containing raster files (`raster_train_file_path`) and defines the chunk size (`CHUNK_SIZE`).\n",
    "\n",
    "2. **`read_raster_in_chunks` Function**:\n",
    "   - Reads a raster file chunk-by-chunk.\n",
    "   - For each chunk, it extracts data from all bands, along with the pixel coordinates.\n",
    "   - Stores each chunk's data in a temporary DataFrame, which is added to a list.\n",
    "\n",
    "3. **Task Creation**:\n",
    "   - Creates a list of tasks using Dask's `@delayed` to process each raster file in parallel.\n",
    "\n",
    "4. **Compute and Combine**:\n",
    "   - Computes each task to get individual DataFrames for each file, then combines them into a final DataFrame (`final_df`).\n",
    "\n",
    "5. **Debug Outputs**:\n",
    "   - Prints the shape, columns, and a sample of the final DataFrame for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the raster files\n",
    "raster_train_file_path = r'Raster_Train'\n",
    "\n",
    "# Parameters for chunk size\n",
    "CHUNK_SIZE = 512 # Adjust following your hardware specification\n",
    "\n",
    "@delayed\n",
    "def read_raster_in_chunks(raster_path, file, root):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        height, width = src.height, src.width\n",
    "        num_bands = src.count\n",
    "        band_names = [f'B{str(i).zfill(2)}' for i in range(1, num_bands + 1)]\n",
    "        chunk_dfs = []\n",
    "        \n",
    "        # Loop over the raster in chunks\n",
    "        for row in range(0, height, CHUNK_SIZE):\n",
    "            for col in range(0, width, CHUNK_SIZE):\n",
    "                window = Window(col_off=col, row_off=row, \n",
    "                              width=min(CHUNK_SIZE, width - col),\n",
    "                              height=min(CHUNK_SIZE, height - row))\n",
    "                \n",
    "                # Read all bands at once\n",
    "                data = src.read(window=window)\n",
    "                \n",
    "                # Check if chunk contains any data\n",
    "                if np.any(data):\n",
    "                    rows, cols = data[0].shape\n",
    "                    \n",
    "                    # Create base DataFrame with coordinates\n",
    "                    row_coords, col_coords = np.meshgrid(\n",
    "                        np.arange(row, row + rows),\n",
    "                        np.arange(col, col + cols),\n",
    "                        indexing=\"ij\"\n",
    "                    )\n",
    "                    \n",
    "                    chunk_df = pd.DataFrame({\n",
    "                        'raster_file': file,\n",
    "                        'subfolder': os.path.basename(root),\n",
    "                        'x': row_coords.flatten(),\n",
    "                        'y': col_coords.flatten()\n",
    "                    })\n",
    "                    \n",
    "                    # Add each band's data\n",
    "                    for band_idx, band_name in enumerate(band_names, 1):\n",
    "                        chunk_df[band_name] = data[band_idx-1].flatten()\n",
    "                    \n",
    "                    chunk_dfs.append(chunk_df)\n",
    "        \n",
    "        return pd.concat(chunk_dfs, ignore_index=True) if chunk_dfs else pd.DataFrame()\n",
    "\n",
    "# Create list of tasks\n",
    "dask_dfs = [\n",
    "    read_raster_in_chunks(os.path.join(root, file), file, root)\n",
    "    for root, dirs, files in os.walk(raster_train_file_path)\n",
    "    for file in files if file.endswith('.tif')\n",
    "]\n",
    "\n",
    "# Compute all tasks\n",
    "dataframes = compute(*dask_dfs)\n",
    "\n",
    "# Combine all DataFrames\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Debug prints\n",
    "print(\"DataFrame shape:\", final_df.shape)\n",
    "print(\"\\nDataFrame columns:\", final_df.columns.tolist())\n",
    "print(\"\\nSample of data:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) & Feature Engineering\n",
    "\n",
    "This code performs several operations on a DataFrame (`final_df`), converting it to a Dask DataFrame for distributed processing, renaming columns, and dropping unnecessary columns:\n",
    "\n",
    "1. **Convert to Dask DataFrame**:\n",
    "   - Converts the existing Pandas DataFrame (`final_df`) into a Dask DataFrame (`ddf`) to enable parallel, distributed processing.\n",
    "\n",
    "2. **Rename Columns**:\n",
    "   - Defines a list of new column names for Sentinel-2 bands and other data (`new_col_names`).\n",
    "   - Renames `ddf` columns using this list to make them more descriptive.\n",
    "\n",
    "3. **Drop Unneeded Columns**:\n",
    "   - Drops columns such as `raster_file`, `subfolder`, `x`, `y`, and `dNBR`, keeping only essential information in the DataFrame (`df`).\n",
    "\n",
    "4. **Display DataFrame**:\n",
    "   - Uses `display()` to view the DataFrame when needed, triggering Dask's computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "new_col_names = ['raster_file', 'subfolder', 'x', 'y', 'Band_1', 'Band_2', 'Band_3', 'Band_4', \n",
    "                 'Band_5', 'Band_6', 'Band_7', 'Band_8', 'Band_8A', 'Band_9', 'Band_11', \n",
    "                 'Band_12', 'dNBR', 'NDVI', 'NDWI', 'Burn_Label']\n",
    "df = final_df.rename(columns=dict(zip(final_df.columns, new_col_names)))\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['raster_file', 'subfolder', 'x', 'y', 'dNBR']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display DataFrame (computing results)\n",
    "print(df.head())  # Displays a small, computed sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Burn Class\n",
    "\n",
    "This code checks and displays the counts of burn records in the DataFrame:\n",
    "\n",
    "1. **Count Burn Labels**:\n",
    "   - Counts occurrences in the `Burn_Label` column to determine the number of \"Burn\" and \"Unburn\" records.\n",
    "   - Renames the labels: `1` to \"Burn\" and `0` to \"Unburn\" for readability.\n",
    "\n",
    "2. **Display Counts**:\n",
    "   - Prints the resulting counts to show the distribution of burn and unburned areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Burn Records\n",
    "burn_counts = df['Burn_Label'].value_counts().rename(index={1: 'Burn', 0: 'Unburn'})\n",
    "\n",
    "# Display the counts with labels\n",
    "print(burn_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "\n",
    "This code performs downsampling to balance the dataset by reducing the number of \"Unburn\" records to match the count of \"Burn\" records:\n",
    "\n",
    "1. **Get Burn Count**:\n",
    "   - Retrieves the count of \"Burn\" records from `burn_counts`.\n",
    "\n",
    "2. **Sample Unburned Records**:\n",
    "   - Selects a random sample of \"Unburn\" records, equal in size to the number of \"Burn\" records, using a fixed `random_state` for reproducibility.\n",
    "\n",
    "3. **Combine Burn and Downsampled Unburn Records**:\n",
    "   - Combines all \"Burn\" records with the downsampled \"Unburn\" sample into a new DataFrame (`downsampled_df`).\n",
    "\n",
    "4. **Check New Burn Record Counts**:\n",
    "   - Counts and displays the \"Burn\" and \"Unburn\" records in `downsampled_df` to verify balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_count = burn_counts['Burn']\n",
    "unburn_sample = df[df['Burn_Label'] == 0].sample(n=burn_count, random_state=42)\n",
    "\n",
    "downsampled_df = pd.concat([df[df['Burn_Label'] == 1], unburn_sample])\n",
    "\n",
    "# Check Burn Records\n",
    "burn_counts = downsampled_df['Burn_Label'].value_counts().rename(index={1: 'Burn', 0: 'Unburn'})\n",
    "\n",
    "# Display the counts with labels\n",
    "print(burn_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove infinite values\n",
    "\n",
    "This code handles the presence of infinite values and missing data in the `downsampled_df` DataFrame:\n",
    "\n",
    "1. **Replace Infinite Values**:\n",
    "   - Replaces both positive and negative infinite values (`np.inf` and `-np.inf`) with `NaN` using `replace()`. This ensures that infinite values do not interfere with further processing.\n",
    "\n",
    "2. **Drop Rows with Missing Values**:\n",
    "   - Removes any rows containing `NaN` values using `dropna()`, ensuring the DataFrame only contains valid data.\n",
    "\n",
    "3. **Display the DataFrame**:\n",
    "   - Displays the cleaned DataFrame (`downsampled_df`) for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing infinite with nan \n",
    "downsampled_df.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "  \n",
    "# Dropping all the rows with nan values \n",
    "downsampled_df.dropna(inplace=True)\n",
    "\n",
    "# Printing df \n",
    "display(downsampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate Burn_Label from DataFrame\n",
    "\n",
    "This code separates the `Burn_Label` from the main DataFrame and ensures the label is in the correct format:\n",
    "\n",
    "1. **Separate Burn Label**:\n",
    "   - Extracts the `Burn_Label` column from `downsampled_df` into a new DataFrame (`burn_label`).\n",
    "\n",
    "2. **Remove Burn Label from Main DataFrame**:\n",
    "   - Drops the `Burn_Label` column from `downsampled_df` to ensure only the feature data remains.\n",
    "\n",
    "3. **Convert Burn Label to Integer**:\n",
    "   - Changes the data type of the `burn_label` DataFrame to `int32` to ensure consistent and efficient processing.\n",
    "\n",
    "4. **Display Burn Label**:\n",
    "   - Displays the modified `burn_label` DataFrame to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Burn_Label from DataFrame\n",
    "burn_label = downsampled_df[['Burn_Label']]\n",
    "\n",
    "# Drop Label from DataFrame\n",
    "downsampled_df = downsampled_df.drop(columns=['Burn_Label'])\n",
    "\n",
    "# Change type of Label to Integer Format\n",
    "burn_label = burn_label.astype('int32')\n",
    "display(burn_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Data with MinMax Scaler\n",
    "\n",
    "This code normalizes the feature data and saves the normalization model, while also combining it with the `Burn_Label`:\n",
    "\n",
    "1. **List Columns**:\n",
    "   - Creates a list of column names from `downsampled_df` to keep track of the column order after normalization (`cols_norm`).\n",
    "\n",
    "2. **Normalize the Data**:\n",
    "   - Imports `MinMaxScaler` from `sklearn` and fits it to the data in `downsampled_df`, which scales the values between 0 and 1.\n",
    "\n",
    "3. **Save the Scaler**:\n",
    "   - Saves the fitted `MinMaxScaler` model to a specified path (`MinMax_Scaler.pkl`) for later use.\n",
    "\n",
    "4. **Apply Normalization**:\n",
    "   - Normalizes the data by applying the `scaler` to `downsampled_df`, then converts the result back into a DataFrame (`df_norm`) with the original column names.\n",
    "\n",
    "5. **Check Shape**:\n",
    "   - Prints the shape of `df_norm` to confirm the normalization was applied correctly.\n",
    "\n",
    "6. **Concatenate with Burn Label**:\n",
    "   - Combines the normalized feature data (`df_norm`) with the `burn_label` DataFrame, aligning them by their indices and ensuring the result is a complete dataset.\n",
    "\n",
    "7. **Display the DataFrame**:\n",
    "   - Displays the final DataFrame (`df_norm`), which now includes both the normalized features and the `Burn_Label`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the dataframe with a list of the columns\n",
    "cols_norm = downsampled_df.columns.tolist()\n",
    "\n",
    "# Import Normalize technique\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize data\n",
    "scaler.fit(downsampled_df)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_save_path = r'Export_Model'\n",
    "save_path = os.path.join(scaler_save_path, 'DL_MinMax_Scaler.pkl')\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Normalize Data\n",
    "df_norm = scaler.transform(downsampled_df)\n",
    "df_norm = pd.DataFrame(df_norm, columns=cols_norm)\n",
    "\n",
    "# Check df_norm shape after normalization\n",
    "print(\"Shape of df_norm after normalization:\", df_norm.shape)\n",
    "\n",
    "# Concatenate df_norm with burn_label\n",
    "df_norm = pd.concat([df_norm.reset_index(drop=True), burn_label.reset_index(drop=True)], axis=1, sort=False)\n",
    "display(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Assuming df_norm is defined and contains 'Burn_Label'\n",
    "X = df_norm.drop(columns=['Burn_Label']).values  # Convert to NumPy array\n",
    "y = df_norm['Burn_Label'].values  # Convert to NumPy array\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to torch tensors and move them to GPU if available\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32)).to(device)\n",
    "        self.y = torch.from_numpy(y.astype(np.float32)).unsqueeze(1).to(device)  # Ensure y is 2D for BCELoss\n",
    "        self.len = self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Instantiate training and test data\n",
    "train_data = Data(X_train, y_train)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(X_test, y_test)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "input_dim = X_train.shape[1]  # Dynamically get input size\n",
    "hidden_dim1 = 128\n",
    "hidden_dim2 = 64\n",
    "output_dim = 1  # Binary classification\n",
    "\n",
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(DeepNeuralNetwork, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)\n",
    "        self.layer_2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.layer_3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.layer_1(x)))\n",
    "        x = torch.relu(self.bn2(self.layer_2(x)))\n",
    "        x = torch.sigmoid(self.layer_3(x))  # Binary classification output\n",
    "        return x\n",
    "\n",
    "# Move model to GPU\n",
    "model = DeepNeuralNetwork(input_dim, hidden_dim1, hidden_dim2, output_dim).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "learning_rate = 0.1\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop with Progress Display\n",
    "num_epochs = 50\n",
    "loss_values = []\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        tepoch.set_description(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for X, y in tepoch:\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            \n",
    "            pred = model(X)  # Forward pass\n",
    "            loss = loss_fn(pred, y)  # Compute loss\n",
    "            \n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=loss.item())  # Show loss in progress bar\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_epoch_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = range(len(loss_values))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(step, np.array(loss_values))\n",
    "plt.title(\"Step-wise Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools  # Import this at the top of your script\n",
    "\n",
    "# Initialize required variables\n",
    "y_pred = []\n",
    "y_test = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X)  # Get model outputs\n",
    "        predicted = np.where(outputs.numpy() < 0.5, 0, 1)  # Convert to NumPy and apply threshold\n",
    "        predicted = list(itertools.chain(*predicted))  # Flatten predictions\n",
    "        y_pred.append(predicted)  # Append predictions\n",
    "        y_test.append(y.numpy())  # Append true labels as NumPy\n",
    "        total += y.size(0)  # Increment total count\n",
    "        correct += (predicted == y.numpy()).sum().item()  # Count correct predictions\n",
    "\n",
    "print(f'Accuracy of the network on the 3300 test instances: {100 * correct // total}%')\n",
    "\n",
    "y_pred = list(itertools.chain(*y_pred))\n",
    "y_test = list(itertools.chain(*y_test))\n",
    "\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "model_result = [{\n",
    "    'Classifier': 'Neural Network',\n",
    "    'Class 0 - Precision': report['0']['precision'],\n",
    "    'Class 0 - Recall': report['0']['recall'],\n",
    "    'Class 0 - F1-Score': report['0']['f1-score'],\n",
    "    'Class 1 - Precision': report['1']['precision'],\n",
    "    'Class 1 - Recall': report['1']['recall'],\n",
    "    'Class 1 - F1-Score': report['1']['f1-score'],\n",
    "    'Average - Precision': report['macro avg']['precision'],\n",
    "    'Average - Recall': report['macro avg']['recall'],\n",
    "    'Average - F1-Score': report['macro avg']['f1-score'],\n",
    "    'Accuracy': report['accuracy'],\n",
    "    'Confusion Matrix': cm\n",
    "}]\n",
    "\n",
    "model_result_df = pd.DataFrame(model_result)\n",
    "\n",
    "display(Markdown(\"### Classification Report of Neural Network\"))\n",
    "display(model_result_df)\n",
    "    \n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix - Neural Network')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FCNN_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
