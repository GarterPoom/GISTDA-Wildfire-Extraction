{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GISTDA Wildfire Machine Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library Packages\n",
    "\n",
    "#### Core Libraries\n",
    "- **numpy**: A powerful library for numerical computing, providing support for arrays, matrices, and a wide range of mathematical operations.\n",
    "- **pandas**: A versatile library for data manipulation and analysis, offering easy-to-use data structures like `DataFrame` for handling tabular data.\n",
    "\n",
    "#### Visualization Libraries\n",
    "- **matplotlib.pyplot**: A plotting library for creating static, animated, and interactive visualizations in Python.\n",
    "- **seaborn**: Built on top of Matplotlib, this library provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "#### Geospatial Libraries\n",
    "- **rasterio**: A library for reading and writing geospatial raster data, commonly used for remote sensing and GIS applications.\n",
    "  - **rasterio.windows.Window**: A tool to define a specific rectangular region (window) within a raster dataset for operations.\n",
    "\n",
    "#### Logging\n",
    "- **logging**: A built-in Python module to record log messages, useful for debugging and monitoring program execution.\n",
    "\n",
    "#### File Handling\n",
    "- **os**: A module providing tools to interact with the operating system, such as reading/writing files and directory management.\n",
    "- **pickle**: A module for serializing and deserializing Python objects for storage or transmission.\n",
    "\n",
    "#### Data Preprocessing\n",
    "- **sklearn.preprocessing.MinMaxScaler**: A preprocessing tool from scikit-learn that scales and normalizes data within a specified range, often [0, 1].\n",
    "\n",
    "#### Display Utilities\n",
    "- **IPython.display.display**: A utility to control the display of outputs in Jupyter notebooks.\n",
    "- **IPython.display.Markdown**: Enables the rendering of Markdown text in a Jupyter notebook.\n",
    "\n",
    "#### Deep Learning Libraries\n",
    "- **tensorflow**: A popular open-source framework for machine learning and deep learning. It supports building and training neural networks efficiently.\n",
    "  - **tensorflow.keras.models.Sequential**: A simple model-building API for stacking layers sequentially.\n",
    "  - **tensorflow.keras.layers.Dense**: A fully connected neural network layer.\n",
    "  - **tensorflow.keras.layers.Dropout**: A regularization technique to prevent overfitting by randomly dropping neurons during training.\n",
    "  - **tensorflow.keras.callbacks.EarlyStopping**: Stops training when a monitored metric stops improving.\n",
    "  - **tensorflow.keras.callbacks.ReduceLROnPlateau**: Adjusts the learning rate when a metric stops improving.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "- **sklearn.metrics.classification_report**: Generates a detailed report of classification metrics, including precision, recall, and F1 score.\n",
    "- **sklearn.metrics.confusion_matrix**: Computes a confusion matrix to evaluate classification accuracy.\n",
    "- **sklearn.metrics.roc_curve**: Computes Receiver Operating Characteristic (ROC) curve metrics.\n",
    "- **sklearn.metrics.auc**: Computes the Area Under the Curve (AUC) for a ROC curve.\n",
    "\n",
    "#### Typing\n",
    "- **typing**: Provides tools for type annotations to improve code clarity and static analysis.\n",
    "  - **Tuple, Dict, Any, List**: Common data types used in type hinting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import rasterio\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import display, Markdown\n",
    "from rasterio.windows import Window\n",
    "from typing import Tuple, Dict, Any, List\n",
    "from dask import delayed, compute\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Raster GeoTIFF files\n",
    "\n",
    "This code reads large raster files in manageable chunks, processes each chunk, and stores the data in a consolidated DataFrame.\n",
    "\n",
    "1. **Folder Path and Chunk Size**: Sets the path to the folder containing raster files (`raster_train_file_path`) and defines the chunk size (`CHUNK_SIZE`).\n",
    "\n",
    "2. **`read_raster_in_chunks` Function**:\n",
    "   - Reads a raster file chunk-by-chunk.\n",
    "   - For each chunk, it extracts data from all bands, along with the pixel coordinates.\n",
    "   - Stores each chunk's data in a temporary DataFrame, which is added to a list.\n",
    "\n",
    "3. **Task Creation**:\n",
    "   - Creates a list of tasks using Dask's `@delayed` to process each raster file in parallel.\n",
    "\n",
    "4. **Compute and Combine**:\n",
    "   - Computes each task to get individual DataFrames for each file, then combines them into a final DataFrame (`final_df`).\n",
    "\n",
    "5. **Debug Outputs**:\n",
    "   - Prints the shape, columns, and a sample of the final DataFrame for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the raster files\n",
    "raster_train_file_path = r'Raster_Train'\n",
    "\n",
    "# Parameters for chunk size\n",
    "CHUNK_SIZE = 1024\n",
    "\n",
    "@delayed\n",
    "def read_raster_in_chunks(raster_path, file, root):\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        height, width = src.height, src.width\n",
    "        num_bands = src.count\n",
    "        band_names = [f'B{str(i).zfill(2)}' for i in range(1, num_bands + 1)]\n",
    "        chunk_dfs = []\n",
    "        \n",
    "        # Loop over the raster in chunks\n",
    "        for row in range(0, height, CHUNK_SIZE):\n",
    "            for col in range(0, width, CHUNK_SIZE):\n",
    "                window = Window(col_off=col, row_off=row, \n",
    "                              width=min(CHUNK_SIZE, width - col),\n",
    "                              height=min(CHUNK_SIZE, height - row))\n",
    "                \n",
    "                # Read all bands at once\n",
    "                data = src.read(window=window)\n",
    "                \n",
    "                # Check if chunk contains any data\n",
    "                if np.any(data):\n",
    "                    rows, cols = data[0].shape\n",
    "                    \n",
    "                    # Create base DataFrame with coordinates\n",
    "                    row_coords, col_coords = np.meshgrid(\n",
    "                        np.arange(row, row + rows),\n",
    "                        np.arange(col, col + cols),\n",
    "                        indexing=\"ij\"\n",
    "                    )\n",
    "                    \n",
    "                    chunk_df = pd.DataFrame({\n",
    "                        'raster_file': file,\n",
    "                        'subfolder': os.path.basename(root),\n",
    "                        'x': row_coords.flatten(),\n",
    "                        'y': col_coords.flatten()\n",
    "                    })\n",
    "                    \n",
    "                    # Add each band's data\n",
    "                    for band_idx, band_name in enumerate(band_names, 1):\n",
    "                        chunk_df[band_name] = data[band_idx-1].flatten()\n",
    "                    \n",
    "                    chunk_dfs.append(chunk_df)\n",
    "        \n",
    "        return pd.concat(chunk_dfs, ignore_index=True) if chunk_dfs else pd.DataFrame()\n",
    "\n",
    "# Create list of tasks\n",
    "dask_dfs = [\n",
    "    read_raster_in_chunks(os.path.join(root, file), file, root)\n",
    "    for root, dirs, files in os.walk(raster_train_file_path)\n",
    "    for file in files if file.endswith('.tif')\n",
    "]\n",
    "\n",
    "# Compute all tasks\n",
    "dataframes = compute(*dask_dfs)\n",
    "\n",
    "# Combine all DataFrames\n",
    "final_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Debug prints\n",
    "print(\"DataFrame shape:\", final_df.shape)\n",
    "print(\"\\nDataFrame columns:\", final_df.columns.tolist())\n",
    "print(\"\\nSample of data:\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) & Feature Engineering\n",
    "\n",
    "This code performs several operations on a DataFrame (`final_df`), converting it to a Dask DataFrame for distributed processing, renaming columns, and dropping unnecessary columns:\n",
    "\n",
    "1. **Convert to Dask DataFrame**:\n",
    "   - Converts the existing Pandas DataFrame (`final_df`) into a Dask DataFrame (`ddf`) to enable parallel, distributed processing.\n",
    "\n",
    "2. **Rename Columns**:\n",
    "   - Defines a list of new column names for Sentinel-2 bands and other data (`new_col_names`).\n",
    "   - Renames `ddf` columns using this list to make them more descriptive.\n",
    "\n",
    "3. **Drop Unneeded Columns**:\n",
    "   - Drops columns such as `raster_file`, `subfolder`, `x`, `y`, and `dNBR`, keeping only essential information in the DataFrame (`df`).\n",
    "\n",
    "4. **Display DataFrame**:\n",
    "   - Uses `display()` to view the DataFrame when needed, triggering Dask's computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Dask DataFrame\n",
    "ddf = pd.DataFrame(final_df)  # Adjust npartitions as needed\n",
    "\n",
    "# Rename Sentinel-2 Bands columns\n",
    "new_col_names = ['raster_file', 'subfolder', 'x', 'y', 'Band_1', 'Band_2', 'Band_3', 'Band_4', 'Band_5', \n",
    "                 'Band_6', 'Band_7', 'Band_8', 'Band_8A', 'Band_9', 'Band_11', 'Band_12', 'dNBR',\n",
    "                 'NDVI', 'NDWI', 'Burn_Label']\n",
    "\n",
    "# Rename columns\n",
    "final_df.columns = new_col_names\n",
    "\n",
    "# Drop columns using Dask method\n",
    "df = final_df.drop(columns=['raster_file', 'subfolder', 'x', 'y', 'dNBR'])\n",
    "display(df)  # Compute only when you need to display or save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Burn Class\n",
    "\n",
    "This code checks and displays the counts of burn records in the DataFrame:\n",
    "\n",
    "1. **Count Burn Labels**:\n",
    "   - Counts occurrences in the `Burn_Label` column to determine the number of \"Burn\" and \"Unburn\" records.\n",
    "   - Renames the labels: `1` to \"Burn\" and `0` to \"Unburn\" for readability.\n",
    "\n",
    "2. **Display Counts**:\n",
    "   - Prints the resulting counts to show the distribution of burn and unburned areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Burn Records\n",
    "burn_counts = df['Burn_Label'].value_counts().rename(index={1: 'Burn', 0: 'Unburn'})\n",
    "\n",
    "# Display the counts with labels\n",
    "print(burn_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "\n",
    "This code performs downsampling to balance the dataset by reducing the number of \"Unburn\" records to match the count of \"Burn\" records:\n",
    "\n",
    "1. **Get Burn Count**:\n",
    "   - Retrieves the count of \"Burn\" records from `burn_counts`.\n",
    "\n",
    "2. **Sample Unburned Records**:\n",
    "   - Selects a random sample of \"Unburn\" records, equal in size to the number of \"Burn\" records, using a fixed `random_state` for reproducibility.\n",
    "\n",
    "3. **Combine Burn and Downsampled Unburn Records**:\n",
    "   - Combines all \"Burn\" records with the downsampled \"Unburn\" sample into a new DataFrame (`downsampled_df`).\n",
    "\n",
    "4. **Check New Burn Record Counts**:\n",
    "   - Counts and displays the \"Burn\" and \"Unburn\" records in `downsampled_df` to verify balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn_count = burn_counts['Burn']\n",
    "unburn_sample = df[df['Burn_Label'] == 0].sample(n=burn_count, random_state=42)\n",
    "\n",
    "downsampled_df = pd.concat([df[df['Burn_Label'] == 1], unburn_sample])\n",
    "\n",
    "# Check Burn Records\n",
    "burn_counts = downsampled_df['Burn_Label'].value_counts().rename(index={1: 'Burn', 0: 'Unburn'})\n",
    "\n",
    "# Display the counts with labels\n",
    "print(burn_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove infinite values\n",
    "\n",
    "This code handles the presence of infinite values and missing data in the `downsampled_df` DataFrame:\n",
    "\n",
    "1. **Replace Infinite Values**:\n",
    "   - Replaces both positive and negative infinite values (`np.inf` and `-np.inf`) with `NaN` using `replace()`. This ensures that infinite values do not interfere with further processing.\n",
    "\n",
    "2. **Drop Rows with Missing Values**:\n",
    "   - Removes any rows containing `NaN` values using `dropna()`, ensuring the DataFrame only contains valid data.\n",
    "\n",
    "3. **Display the DataFrame**:\n",
    "   - Displays the cleaned DataFrame (`downsampled_df`) for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing infinite with nan \n",
    "downsampled_df.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "  \n",
    "# Dropping all the rows with nan values \n",
    "downsampled_df.dropna(inplace=True)\n",
    "\n",
    "# Printing df \n",
    "display(downsampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate Burn_Label from DataFrame\n",
    "\n",
    "This code separates the `Burn_Label` from the main DataFrame and ensures the label is in the correct format:\n",
    "\n",
    "1. **Separate Burn Label**:\n",
    "   - Extracts the `Burn_Label` column from `downsampled_df` into a new DataFrame (`burn_label`).\n",
    "\n",
    "2. **Remove Burn Label from Main DataFrame**:\n",
    "   - Drops the `Burn_Label` column from `downsampled_df` to ensure only the feature data remains.\n",
    "\n",
    "3. **Convert Burn Label to Integer**:\n",
    "   - Changes the data type of the `burn_label` DataFrame to `int32` to ensure consistent and efficient processing.\n",
    "\n",
    "4. **Display Burn Label**:\n",
    "   - Displays the modified `burn_label` DataFrame to verify the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate Burn_Label from DataFrame\n",
    "burn_label = downsampled_df[['Burn_Label']]\n",
    "\n",
    "# Drop Label from DataFrame\n",
    "downsampled_df = downsampled_df.drop(columns=['Burn_Label'])\n",
    "\n",
    "# Change type of Label to Integer Format\n",
    "burn_label = burn_label.astype('int32')\n",
    "display(burn_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization Data with MinMax Scaler\n",
    "\n",
    "This code normalizes the feature data and saves the normalization model, while also combining it with the `Burn_Label`:\n",
    "\n",
    "1. **List Columns**:\n",
    "   - Creates a list of column names from `downsampled_df` to keep track of the column order after normalization (`cols_norm`).\n",
    "\n",
    "2. **Normalize the Data**:\n",
    "   - Imports `MinMaxScaler` from `sklearn` and fits it to the data in `downsampled_df`, which scales the values between 0 and 1.\n",
    "\n",
    "3. **Save the Scaler**:\n",
    "   - Saves the fitted `MinMaxScaler` model to a specified path (`MinMax_Scaler.pkl`) for later use.\n",
    "\n",
    "4. **Apply Normalization**:\n",
    "   - Normalizes the data by applying the `scaler` to `downsampled_df`, then converts the result back into a DataFrame (`df_norm`) with the original column names.\n",
    "\n",
    "5. **Check Shape**:\n",
    "   - Prints the shape of `df_norm` to confirm the normalization was applied correctly.\n",
    "\n",
    "6. **Concatenate with Burn Label**:\n",
    "   - Combines the normalized feature data (`df_norm`) with the `burn_label` DataFrame, aligning them by their indices and ensuring the result is a complete dataset.\n",
    "\n",
    "7. **Display the DataFrame**:\n",
    "   - Displays the final DataFrame (`df_norm`), which now includes both the normalized features and the `Burn_Label`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the dataframe with a list of the columns\n",
    "cols_norm = downsampled_df.columns.tolist()\n",
    "\n",
    "# Import Normalize technique\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize data\n",
    "scaler.fit(downsampled_df)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_save_path = r'Export Model'\n",
    "save_path = os.path.join(scaler_save_path, 'MinMax_Scaler_DL.pkl')\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Normalize Data\n",
    "df_norm = scaler.transform(downsampled_df)\n",
    "df_norm = pd.DataFrame(df_norm, columns=cols_norm)\n",
    "\n",
    "# Check df_norm shape after normalization\n",
    "print(\"Shape of df_norm after normalization:\", df_norm.shape)\n",
    "\n",
    "# Concatenate df_norm with burn_label\n",
    "df_norm = pd.concat([df_norm.reset_index(drop=True), burn_label.reset_index(drop=True)], axis=1, sort=False)\n",
    "display(df_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Hyperparameters Analysis & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check if GPU is available\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    logger.info(\"GPU is available and will be used for training.\")\n",
    "    gpu_name = gpus[0].name\n",
    "    logger.info(f\"GPU: {gpu_name}\")\n",
    "else:\n",
    "    logger.info(\"GPU is not available, training will be done on CPU.\")\n",
    "\n",
    "# Ensure df_norm is already defined and preprocessed\n",
    "# Define the features and target\n",
    "X = df_norm.drop(columns=['Burn_Label'])  # Features\n",
    "y = df_norm['Burn_Label']  # Target\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_onehot = tf.keras.utils.to_categorical(y, num_classes=y.nunique())\n",
    "\n",
    "# Function to build the model\n",
    "def build_model(hidden_layers=3):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=(X.shape[1],)))\n",
    "    for _ in range(hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(y_onehot.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Grid Search for Best Hyperparameters\n",
    "best_params = None\n",
    "best_score = 0\n",
    "\n",
    "# Train the Model with Best Hyperparameters on Entire Dataset\n",
    "model = build_model(hidden_layers=3)\n",
    "history = model.fit(X, y_onehot, epochs=5, batch_size=32, verbose=1)\n",
    "\n",
    "# Plot Accuracy and Loss Graphs\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Cross-Validation Predictions\n",
    "y_pred = cross_val_predict(\n",
    "    estimator=tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn=lambda: build_model(hidden_layers=3),\n",
    "                                                             epochs=2,\n",
    "                                                             batch_size=32, verbose=1),\n",
    "    X=X, y=y, cv=10\n",
    ")\n",
    "\n",
    "# Classification Report as Pandas DataFrame\n",
    "report = classification_report(y, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "print(report_df)\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FCNN-GPUENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
